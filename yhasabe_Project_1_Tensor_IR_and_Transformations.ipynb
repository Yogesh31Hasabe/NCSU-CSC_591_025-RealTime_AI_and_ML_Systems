{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oKpzqu-nS2L"
      },
      "source": [
        "Project ~ 1 : Tensor IR and Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcBRYkKnMhAb"
      },
      "source": [
        "## Install Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4IKvnOwJAcv"
      },
      "source": [
        "Install MLC Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik0ojS3RnPfr",
        "outputId": "024ee662-4973-4c8a-afeb-feed9bd49db0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-cpu\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_cpu-0.17.2-cp310-cp310-manylinux_2_28_x86_64.whl (185.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.8/185.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (24.2.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (2.2.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (4.4.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (1.13.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (6.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (4.12.2)\n",
            "Installing collected packages: mlc-ai-cpu\n",
            "Successfully installed mlc-ai-cpu-0.17.2\n"
          ]
        }
      ],
      "source": [
        "!python3 -m  pip install mlc-ai-cpu -f https://mlc.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6padCbkFJRcg"
      },
      "source": [
        "Install required python and torch packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJFrc0hRuMs-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision.datasets import *\n",
        "from torchvision.transforms import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-9dGTQtJZqN"
      },
      "source": [
        "Install required TVM packages & libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVq10hxSsA78"
      },
      "outputs": [],
      "source": [
        "import tvm\n",
        "from tvm import te, auto_scheduler\n",
        "from tvm.ir.module import IRModule\n",
        "from tvm.script import tir as T, relax as R\n",
        "from tvm import relax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq-emHq6MHqG"
      },
      "source": [
        "## Define the architecture Original DNN Model and load the DNN Model of [Project 0](https://colab.research.google.com/drive/1chvZmVvxvW8G3VQlmLaExoTB1CIfu7Ph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2OlPI-bJeOB"
      },
      "source": [
        "Defining the Original DNN Model Architecture referenced in [Project 0](https://colab.research.google.com/drive/1chvZmVvxvW8G3VQlmLaExoTB1CIfu7Ph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLjp-iQ5CTiJ"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.fc1 = nn.Linear(in_features=32*8*8, out_features=128)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.bn1(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.bn2(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 32*8*8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiLaUkxGJ_pV"
      },
      "source": [
        "Load the saved DNN Model of [Project 0](https://colab.research.google.com/drive/1chvZmVvxvW8G3VQlmLaExoTB1CIfu7Ph) & its parameters too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNgSCVlNU7B3",
        "outputId": "1344fe88-9457-4771-b9e5-ca7c668ea90c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-10-04 00:14:47--  https://github.com/Yogesh31Hasabe/NCSU-CSC_591_025-RealTime_AI_and_ML_Systems/raw/main/DNNTutorial_0_Model.pkl\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Yogesh31Hasabe/NCSU-CSC_591_025-RealTime_AI_and_ML_Systems/main/DNNTutorial_0_Model.pkl [following]\n",
            "--2024-10-04 00:14:47--  https://raw.githubusercontent.com/Yogesh31Hasabe/NCSU-CSC_591_025-RealTime_AI_and_ML_Systems/main/DNNTutorial_0_Model.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1080864 (1.0M) [application/octet-stream]\n",
            "Saving to: ‘DNNTutorial_0_Model.pkl’\n",
            "\n",
            "DNNTutorial_0_Model 100%[===================>]   1.03M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2024-10-04 00:14:47 (112 MB/s) - ‘DNNTutorial_0_Model.pkl’ saved [1080864/1080864]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O DNNTutorial_0_Model.pkl https://github.com/Yogesh31Hasabe/NCSU-CSC_591_025-RealTime_AI_and_ML_Systems/raw/main/DNNTutorial_0_Model.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf_2OT_VnVGV",
        "outputId": "781529dd-2318-4730-99a4-59ca4ee6c629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = SimpleCNN()\n",
        "mlp_params = torch.load(\"DNNTutorial_0_Model.pkl\", map_location=torch.device('cpu'), weights_only=True)\n",
        "# print(mlp_params.keys())\n",
        "model.load_state_dict(mlp_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFTK2P_XLsIU"
      },
      "source": [
        "## Load & infer the CIFAR-10 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxL6K67KKbmW"
      },
      "source": [
        "Load the CIFAR-10 test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR2UeA9rlSyC",
        "outputId": "5e133b1f-ef39-45b4-e1d5-a5581b0e24ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 70811795.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Label: cat\n"
          ]
        }
      ],
      "source": [
        "# Create a DataLoader for the test set\n",
        "BATCH_SIZE = 1\n",
        "test_data = torchvision.datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "# CIFAR-10 class names\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Get one image and label from the test set\n",
        "img, label = next(iter(test_loader))\n",
        "img1 = img.squeeze(0).permute(1, 2, 0).numpy()\n",
        "\n",
        "# Display the label for the image\n",
        "print(f\"Label: {class_names[label]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxy9J2VnK_5d"
      },
      "source": [
        "Display the Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "69ClSxziK_Im",
        "outputId": "a67c0f0d-a8c2-4018-ea7e-433ef0a3135f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class: cat\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(img1)\n",
        "plt.axis('off')\n",
        "print(\"Class:\", class_names[label[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzBO25gbLGlW"
      },
      "source": [
        "Perform inference with the DNN model of [Project 0](https://colab.research.google.com/drive/1chvZmVvxvW8G3VQlmLaExoTB1CIfu7Ph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzokoo0tLxfY"
      },
      "source": [
        "## Inference of saved DNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmyGX5a4-GmX",
        "outputId": "49328f96-df2e-4349-ff96-ee336e13cd89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted class: cat\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "  pred = model(img)\n",
        "\n",
        "\n",
        "# Get the predicted class index and name\n",
        "predicted_class_index = pred.argmax(dim=1).item()\n",
        "predicted_class_name = class_names[predicted_class_index]\n",
        "print(f'Predicted class: {predicted_class_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcOg0x6NM20d"
      },
      "source": [
        "## Define the dimensions & initialize the placeholders for all the corresponding DNN blocks and layers using `te.placeholder`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2fOkmuBhQOO"
      },
      "outputs": [],
      "source": [
        "# Define the sizes of the various CNN blocks\n",
        "\n",
        "# Batch size, input channels, height, width\n",
        "N, C, H, W = BATCH_SIZE, 3, 32, 32\n",
        "\n",
        "# Number of filters\n",
        "K1, K2 = 16, 32\n",
        "\n",
        "# Kernel height and width\n",
        "R, S = 3, 3\n",
        "\n",
        "# Strides for height and width\n",
        "stride_h, stride_w = 1, 1\n",
        "\n",
        "# Padding for height and width\n",
        "padding_h, padding_w = 1, 1\n",
        "\n",
        "# Number of fully connected nodes\n",
        "FCC1, FCC2 = 128, 10\n",
        "\n",
        "\n",
        "# Placeholders for Convolution layers\n",
        "Input = te.placeholder((N, C, H, W), dtype=\"float32\", name=\"Input\")\n",
        "Kernel1 = te.placeholder((K1, C, R, S), dtype=\"float32\", name=\"Kernel1\")\n",
        "Kernel2 = te.placeholder((K2, K1, R, S), dtype=\"float32\", name=\"Kernel2\")\n",
        "Bias_conv1 = te.placeholder((K1,), dtype=\"float32\", name=\"Bias_conv_1\")\n",
        "Bias_conv2 = te.placeholder((K2,), dtype=\"float32\", name=\"Bias_conv_2\")\n",
        "\n",
        "\n",
        "# Placeholders for Batch Normalization layers\n",
        "BN1_weight = te.placeholder((K1,), dtype=\"float32\", name=\"BN1_weight\")\n",
        "BN1_bias = te.placeholder((K1,), dtype=\"float32\", name=\"BN1_bias\")\n",
        "BN1_mean = te.placeholder((K1,), dtype=\"float32\", name=\"BN1_mean\")\n",
        "BN1_var = te.placeholder((K1,), dtype=\"float32\", name=\"BN1_var\")\n",
        "BN2_weight = te.placeholder((K2,), dtype=\"float32\", name=\"BN2_weight\")\n",
        "BN2_bias = te.placeholder((K2,), dtype=\"float32\", name=\"BN2_bias\")\n",
        "BN2_mean = te.placeholder((K2,), dtype=\"float32\", name=\"BN2_mean\")\n",
        "BN2_var = te.placeholder((K2,), dtype=\"float32\", name=\"BN2_var\")\n",
        "\n",
        "\n",
        "# Placeholders for Fully Connected layers\n",
        "Weight1 = te.placeholder((FCC1, K2 * 8 * 8), dtype=\"float32\", name=\"Weight1\")\n",
        "Bias1 = te.placeholder((FCC1,), dtype=\"float32\", name=\"Bias1\")\n",
        "Weight2 = te.placeholder((FCC2, FCC1), dtype=\"float32\", name=\"Weight2\")\n",
        "Bias2 = te.placeholder((FCC2,), dtype=\"float32\", name=\"Bias2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41QU-dTeLSTP"
      },
      "source": [
        "#1. CNN Method Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooKc-pSMRR7j"
      },
      "source": [
        "## Tensor IR Implemetation for the Direct Convolution (CNN) method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-W829agNU3v"
      },
      "source": [
        "Define the CNN function containing the flow of CNN operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIXM1XaUg9qv"
      },
      "outputs": [],
      "source": [
        "def cnn_original(Input, Kernels, Bias_conv, BNWeight, BNBias, BNMean, BNVariance, Weights, Bias, num_classes=10):\n",
        "\n",
        "    def Conv2D(Input, Kernel, Bias, stride=1, padding=1):\n",
        "        N, C, H, W = Input.shape\n",
        "        K, _, R, S = Kernel.shape\n",
        "        P = (H - R + 2 * padding) // stride + 1\n",
        "        Q = (W - S + 2 * padding) // stride + 1\n",
        "\n",
        "        PaddedInput = te.compute(\n",
        "            (N, C, H + 2 * padding, W + 2 * padding),\n",
        "            lambda n, c, h, w: tvm.tir.if_then_else(\n",
        "                tvm.tir.all(h >= padding, h < H + padding, w >= padding, w < W + padding),\n",
        "                Input[n, c, h - padding, w - padding],\n",
        "                0.0\n",
        "            ),\n",
        "            name=\"PaddedInput\"\n",
        "        )\n",
        "\n",
        "        rc = te.reduce_axis((0, C), name=\"rc\")\n",
        "        ry = te.reduce_axis((0, R), name=\"ry\")\n",
        "        rx = te.reduce_axis((0, S), name=\"rx\")\n",
        "\n",
        "        Conv = te.compute(\n",
        "            (N, K, P, Q),\n",
        "            lambda n, k, p, q: te.sum(\n",
        "                PaddedInput[n, rc, p * stride + ry, q * stride + rx] * Kernel[k, rc, ry, rx],\n",
        "                axis=[rc, ry, rx]\n",
        "            ),\n",
        "            name=\"Conv\"\n",
        "        )\n",
        "\n",
        "        ConvBias = te.compute(\n",
        "            (N, K, P, Q),\n",
        "            lambda n, k, p, q: Conv[n, k, p, q] + Bias[k],\n",
        "            name=\"ConvBias\"\n",
        "        )\n",
        "\n",
        "        return ConvBias\n",
        "\n",
        "    def ReLU(Input):\n",
        "        return te.compute(\n",
        "            Input.shape,\n",
        "            lambda n, c, h, w: te.max(Input[n, c, h, w], 0.0),\n",
        "            name=\"ReLU\"\n",
        "        )\n",
        "\n",
        "    def BatchNormalization(Input, BNWeight, BNBias, BNMean, BNVariance, epsilon=1e-5):\n",
        "        return te.compute(\n",
        "            Input.shape,\n",
        "            lambda n, c, h, w: (Input[n, c, h, w] - BNMean[c]) / te.sqrt(BNVariance[c] + epsilon) * BNWeight[c] + BNBias[c],\n",
        "            name=\"BatchNorm\"\n",
        "        )\n",
        "\n",
        "    def MaxPool2D(Input, kernel_size=2):\n",
        "        N, C, H, W = Input.shape\n",
        "        P = H // kernel_size\n",
        "        Q = W // kernel_size\n",
        "\n",
        "        rph = te.reduce_axis((0, kernel_size), name=\"rph\")\n",
        "        rpw = te.reduce_axis((0, kernel_size), name=\"rpw\")\n",
        "\n",
        "        Pooled = te.compute(\n",
        "            (N, C, P, Q),\n",
        "            lambda n, c, h, w: te.max(\n",
        "                Input[n, c, h * kernel_size + rph, w * kernel_size + rpw],\n",
        "                axis=[rph, rpw]\n",
        "            ),\n",
        "            name=\"Pooled\"\n",
        "        )\n",
        "\n",
        "        return Pooled\n",
        "\n",
        "    def Flatten(Input):\n",
        "        N, C, H, W = Input.shape\n",
        "        return te.compute(\n",
        "            (N, C * H * W),\n",
        "            lambda n, i: Input[n, i // (H * W), (i // W) % H, i % W],\n",
        "            name=\"Flattened\"\n",
        "        )\n",
        "\n",
        "    def LinearReLU(Input, Weight, Bias):\n",
        "        N, I = Input.shape\n",
        "        O, _ = Weight.shape\n",
        "\n",
        "        r = te.reduce_axis((0, I), name=\"r\")\n",
        "        Linear = te.compute(\n",
        "            (N, O),\n",
        "            lambda n, o: te.sum(Input[n, r] * Weight[o, r], axis=r),\n",
        "            name=\"Linear\"\n",
        "        )\n",
        "\n",
        "        return te.compute(\n",
        "            (N, O),\n",
        "            lambda n, o: te.max(Linear[n, o] + Bias[o], 0.0),\n",
        "            name=\"LinearReLU\"\n",
        "        )\n",
        "\n",
        "    def Linear(Input, Weight, Bias):\n",
        "        N, I = Input.shape\n",
        "        O, _ = Weight.shape\n",
        "\n",
        "        r = te.reduce_axis((0, I), name=\"r\")\n",
        "        Linear = te.compute(\n",
        "            (N, O),\n",
        "            lambda n, o: te.sum(Input[n, r] * Weight[o, r], axis=r),\n",
        "            name=\"Linear\"\n",
        "        )\n",
        "\n",
        "        return te.compute(\n",
        "            (N, O),\n",
        "            lambda n, o: Linear[n, o] + Bias[o],\n",
        "            name=\"LinearBias\"\n",
        "        )\n",
        "\n",
        "    # Forward pass\n",
        "    conv_1 = Conv2D(Input, Kernels[0], Bias_conv[0])\n",
        "    relu_1 = ReLU(conv_1)\n",
        "    bn_1 = BatchNormalization(relu_1, BNWeight[0], BNBias[0], BNMean[0], BNVariance[0])\n",
        "    maxpool2d_1 = MaxPool2D(bn_1)\n",
        "\n",
        "    conv_2 = Conv2D(maxpool2d_1, Kernels[1], Bias_conv[1])\n",
        "    relu_2 = ReLU(conv_2)\n",
        "    bn_2 = BatchNormalization(relu_2, BNWeight[1], BNBias[1], BNMean[1], BNVariance[1])\n",
        "    maxpool2d_2 = MaxPool2D(bn_2)\n",
        "\n",
        "    flatten = Flatten(maxpool2d_2)\n",
        "\n",
        "    fc1 = LinearReLU(flatten, Weights[0], Bias[0])\n",
        "    fc2 = Linear(fc1, Weights[1], Bias[1])\n",
        "\n",
        "    s = te.create_schedule(fc2.op)\n",
        "    return s, conv_1, conv_2, fc1, fc2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zta5J_b_Nofn"
      },
      "source": [
        "Create the CNN schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrAo3ms1hk1S"
      },
      "outputs": [],
      "source": [
        "s, conv_1, conv_2, fc1, fc2 = cnn_original(Input, [Kernel1, Kernel2], [Bias_conv1, Bias_conv2], [BN1_weight, BN2_weight], [BN1_bias, BN2_bias], [BN1_mean, BN2_mean], [BN1_var, BN2_var], [Weight1, Weight2], [Bias1, Bias2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ-OsXu1N1wM"
      },
      "source": [
        "Create TIR & Show the IRModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q_KemyR4hrlg",
        "outputId": "02fe4733-6634-4921-e8a4-32edc3f6497e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">cnn_original</span>(Input: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Kernel1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Kernel2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias_conv_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias_conv_2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Weight1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">2048</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Weight2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">10</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_weight: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_bias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_mean: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_var: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_weight: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_bias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_mean: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_var: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), LinearBias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        PaddedInput <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">34</span>, <span style=\"color: #008000\">34</span>))\n",
              "        Conv <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        ConvBias <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        ReLU <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        BatchNorm <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        Pooled <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        PaddedInput_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">18</span>, <span style=\"color: #008000\">18</span>))\n",
              "        Conv_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        ConvBias_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        ReLU_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        BatchNorm_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        Pooled_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">8</span>))\n",
              "        Flattened <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2048</span>))\n",
              "        Linear <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>))\n",
              "        LinearReLU <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>))\n",
              "        Linear_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">34</span>, <span style=\"color: #008000\">34</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;PaddedInput&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Input[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(PaddedInput[v_n, v_c, v_h, v_w])\n",
              "                PaddedInput[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_h <span style=\"color: #008000; font-weight: bold\">and</span> v_h <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">33</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_w <span style=\"color: #008000; font-weight: bold\">and</span> v_w <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">33</span>, Input[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q, rc, ry, rx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Conv&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q, v_rc, v_ry, v_rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [n, k, p, q, rc, ry, rx])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(PaddedInput[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx], Kernel1[v_k, v_rc, v_ry, v_rx])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Conv[v_n, v_k, v_p, v_q])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> PaddedInput[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Kernel1[v_k, v_rc, v_ry, v_rx]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ConvBias&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, k, p, q])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Conv[v_n, v_k, v_p, v_q], Bias_conv_1[v_k])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ConvBias[v_n, v_k, v_p, v_q])\n",
              "                ConvBias[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias_conv_1[v_k]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ReLU&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ConvBias[v_n, v_c, v_h, v_w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ReLU[v_n, v_c, v_h, v_w])\n",
              "                ReLU[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(ConvBias[v_n, v_c, v_h, v_w], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;BatchNorm&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ReLU[v_n, v_c, v_h, v_w], BN1_mean[v_c], BN1_var[v_c], BN1_weight[v_c], BN1_bias[v_c])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(BatchNorm[v_n, v_c, v_h, v_w])\n",
              "                BatchNorm[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> (ReLU[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">-</span> BN1_mean[v_c]) <span style=\"color: #AA22FF; font-weight: bold\">/</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>sqrt(BN1_var[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">1.0000000000000001e-05</span>)) <span style=\"color: #AA22FF; font-weight: bold\">*</span> BN1_weight[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> BN1_bias[v_c]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w, rph, rpw <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Pooled&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w, v_rph, v_rpw <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRR&quot;</span>, [n, c, h, w, rph, rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(BatchNorm[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Pooled[v_n, v_c, v_h, v_w])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Pooled[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #AA22FF; font-weight: bold\">-</span><span style=\"color: #008000\">340282346638528859811704183484516925440.0</span>)\n",
              "                Pooled[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Pooled[v_n, v_c, v_h, v_w], BatchNorm[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">18</span>, <span style=\"color: #008000\">18</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;PaddedInput_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Pooled[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(PaddedInput_1[v_n, v_c, v_h, v_w])\n",
              "                PaddedInput_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_h <span style=\"color: #008000; font-weight: bold\">and</span> v_h <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">17</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_w <span style=\"color: #008000; font-weight: bold\">and</span> v_w <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">17</span>, Pooled[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q, rc, ry, rx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Conv_1&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q, v_rc, v_ry, v_rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [n, k, p, q, rc, ry, rx])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(PaddedInput_1[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx], Kernel2[v_k, v_rc, v_ry, v_rx])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Conv_1[v_n, v_k, v_p, v_q])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> PaddedInput_1[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Kernel2[v_k, v_rc, v_ry, v_rx]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ConvBias_1&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, k, p, q])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Conv_1[v_n, v_k, v_p, v_q], Bias_conv_2[v_k])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ConvBias_1[v_n, v_k, v_p, v_q])\n",
              "                ConvBias_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias_conv_2[v_k]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ReLU_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ConvBias_1[v_n, v_c, v_h, v_w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ReLU_1[v_n, v_c, v_h, v_w])\n",
              "                ReLU_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(ConvBias_1[v_n, v_c, v_h, v_w], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;BatchNorm_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ReLU_1[v_n, v_c, v_h, v_w], BN2_mean[v_c], BN2_var[v_c], BN2_weight[v_c], BN2_bias[v_c])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(BatchNorm_1[v_n, v_c, v_h, v_w])\n",
              "                BatchNorm_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> (ReLU_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">-</span> BN2_mean[v_c]) <span style=\"color: #AA22FF; font-weight: bold\">/</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>sqrt(BN2_var[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">1.0000000000000001e-05</span>)) <span style=\"color: #AA22FF; font-weight: bold\">*</span> BN2_weight[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> BN2_bias[v_c]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w, rph, rpw <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Pooled_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w, v_rph, v_rpw <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRR&quot;</span>, [n, c, h, w, rph, rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(BatchNorm_1[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Pooled_1[v_n, v_c, v_h, v_w])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Pooled_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #AA22FF; font-weight: bold\">-</span><span style=\"color: #008000\">340282346638528859811704183484516925440.0</span>)\n",
              "                Pooled_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Pooled_1[v_n, v_c, v_h, v_w], BatchNorm_1[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, i <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2048</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Flattened&quot;</span>):\n",
              "                v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Pooled_1[v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">64</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span>])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Flattened[v_n, v_i])\n",
              "                Flattened[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Pooled_1[v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">64</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span>]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o, r <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">2048</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Linear&quot;</span>):\n",
              "                v_n, v_o, v_r <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [n, o, r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Flattened[v_n, v_r], Weight1[v_o, v_r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Linear[v_n, v_o])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Flattened[v_n, v_r] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Weight1[v_o, v_r]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;LinearReLU&quot;</span>):\n",
              "                v_n, v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Linear[v_n, v_o], Bias1[v_o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(LinearReLU[v_n, v_o])\n",
              "                LinearReLU[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias1[v_o], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o, r <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Linear_1&quot;</span>):\n",
              "                v_n, v_o, v_r <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [n, o, r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(LinearReLU[v_n, v_r], Weight2[v_o, v_r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Linear_1[v_n, v_o])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> LinearReLU[v_n, v_r] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Weight2[v_o, v_r]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;LinearBias&quot;</span>):\n",
              "                v_n, v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Linear_1[v_n, v_o], Bias2[v_o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(LinearBias[v_n, v_o])\n",
              "                LinearBias[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias2[v_o]\n",
              "</pre></div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "te_func = te.create_prim_func([\n",
        "    Input, Kernel1, Kernel2, Bias_conv1, Bias_conv2,\n",
        "    Weight1, Bias1, Weight2, Bias2,\n",
        "    BN1_weight, BN1_bias, BN1_mean, BN1_var,\n",
        "    BN2_weight, BN2_bias, BN2_mean, BN2_var,\n",
        "    fc2\n",
        "]).with_attr({\"global_symbol\": \"cnn_original\"})\n",
        "\n",
        "Original_Module = tvm.IRModule({\"cnn_original\": te_func})\n",
        "Original_Module.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIYQAwZ5OU-n"
      },
      "source": [
        "Compile & build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9L1WLAkhw1Q"
      },
      "outputs": [],
      "source": [
        "module = tvm.build(Original_Module, [\n",
        "    Input, Kernel1, Kernel2, Bias_conv1, Bias_conv2,\n",
        "    Weight1, Bias1, Weight2, Bias2,\n",
        "    BN1_weight, BN1_bias, BN1_mean, BN1_var,\n",
        "    BN2_weight, BN2_bias, BN2_mean, BN2_var,\n",
        "    fc2\n",
        "], target=\"llvm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cw5rSNFOqiX"
      },
      "source": [
        "Retrieving the model parameters from the saved DNN model and initializing the placeholder variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMKNofu_h4SW"
      },
      "outputs": [],
      "source": [
        "# Parameters for Layer 1\n",
        "kernel1_data = mlp_params['conv1.weight'].numpy()\n",
        "bias_conv1_data = mlp_params['conv1.bias'].numpy()\n",
        "bn1_weight_data = mlp_params['bn1.weight'].numpy()\n",
        "bn1_bias_data = mlp_params['bn1.bias'].numpy()\n",
        "bn1_mean_data = mlp_params['bn1.running_mean'].numpy()\n",
        "bn1_var_data = mlp_params['bn1.running_var'].numpy()\n",
        "\n",
        "# Parameters for Layer 2\n",
        "kernel2_data = mlp_params['conv2.weight'].numpy()\n",
        "bias_conv2_data = mlp_params['conv2.bias'].numpy()\n",
        "bn2_weight_data = mlp_params['bn2.weight'].numpy()\n",
        "bn2_bias_data = mlp_params['bn2.bias'].numpy()\n",
        "bn2_mean_data = mlp_params['bn2.running_mean'].numpy()\n",
        "bn2_var_data = mlp_params['bn2.running_var'].numpy()\n",
        "\n",
        "# Parameters for Layer 3\n",
        "weight1_data = mlp_params['fc1.weight'].numpy()\n",
        "bias1_data = mlp_params['fc1.bias'].numpy()\n",
        "weight2_data = mlp_params['fc2.weight'].numpy()\n",
        "bias2_data = mlp_params['fc2.bias'].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNUNtoK7O_I2"
      },
      "source": [
        "Converting the numpy array to `tvm.nd.array`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBMEz1bmh4t_"
      },
      "outputs": [],
      "source": [
        "ctx = tvm.cpu(0)\n",
        "config = [\n",
        "    tvm.nd.array(kernel1_data, ctx),\n",
        "    tvm.nd.array(kernel2_data, ctx),\n",
        "    tvm.nd.array(bias_conv1_data, ctx),\n",
        "    tvm.nd.array(bias_conv2_data, ctx),\n",
        "    tvm.nd.array(weight1_data, ctx),\n",
        "    tvm.nd.array(bias1_data, ctx),\n",
        "    tvm.nd.array(weight2_data, ctx),\n",
        "    tvm.nd.array(bias2_data, ctx),\n",
        "    tvm.nd.array(bn1_weight_data, ctx),\n",
        "    tvm.nd.array(bn1_bias_data, ctx),\n",
        "    tvm.nd.array(bn1_mean_data, ctx),\n",
        "    tvm.nd.array(bn1_var_data, ctx),\n",
        "    tvm.nd.array(bn2_weight_data, ctx),\n",
        "    tvm.nd.array(bn2_bias_data, ctx),\n",
        "    tvm.nd.array(bn2_mean_data, ctx),\n",
        "    tvm.nd.array(bn2_var_data, ctx),\n",
        "]\n",
        "\n",
        "output_tvm = tvm.nd.empty((N, 10), device=ctx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51WCyRC1PJwd"
      },
      "source": [
        "## Inference & Testing Correctness for Original Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip5RTqKi1_xX"
      },
      "source": [
        "### Inference & Testing Correctness: `Accuracy %` on Test Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM0U0hmDPUIx"
      },
      "source": [
        "Define the inference function to calculate the accuracy for the test dataset (CIFAR-10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS3MZDAkiAr0",
        "outputId": "cae4a575-2d27-4c3c-8fe7-129ca9853358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Accuracy is : 79.17 %\n"
          ]
        }
      ],
      "source": [
        "def inference(module, test_loader, config, output_tvm):\n",
        "  ctx = tvm.cpu(0)\n",
        "  num_samples = 0\n",
        "  num_correct = 0\n",
        "\n",
        "  for inputs, targets in test_loader:\n",
        "    input_tvm = tvm.nd.array(inputs, ctx)\n",
        "    module(input_tvm, *config, output_tvm)\n",
        "    output_numpy = output_tvm.asnumpy()\n",
        "    num_samples += targets.size(0)\n",
        "    num_correct += (np.argmax(output_numpy[0]) == targets.numpy()[0]).sum()\n",
        "\n",
        "  print(f\"The Accuracy is : {(num_correct / num_samples * 100).item()} %\")\n",
        "  return input_tvm\n",
        "\n",
        "input_tvm = inference(module, test_loader, config, output_tvm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plZndZmoP3TL"
      },
      "source": [
        "### Inference & Testing Correctness: `Prediction` for Single Instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46M2bWb5puYO",
        "outputId": "0124c808-c18d-44d8-d633-1c18f401fa22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Module Prediction is: cat\n"
          ]
        }
      ],
      "source": [
        "input_tvm = tvm.nd.array(img, ctx)\n",
        "input_tvm.shape\n",
        "module(input_tvm, *config, output_tvm)\n",
        "pred_kind = np.argmax(output_tvm.numpy(), axis=1)\n",
        "print(\"Original Module Prediction is:\", class_names[pred_kind[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM6_psCDP8Le"
      },
      "source": [
        "### Inference & Testing Correctness: `Assertion` Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMB505vmqKC2",
        "outputId": "c42c27a2-049e-4953-c045-e46279dc56f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correctness Test Passed.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    np.testing.assert_allclose(pred.numpy(), output_tvm.asnumpy(), rtol=1e-4)\n",
        "    print(\"Correctness Test Passed.\")\n",
        "except AssertionError as e:\n",
        "    print(f\"AssertionError: {e}\")\n",
        "    print(\"Correctness Test Failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8u_cT0DQPFV"
      },
      "source": [
        "## Execution Time for Original Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3zOmhPBAGjl",
        "outputId": "216f9f97-f375-4084-8cbc-1600604608ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time for Original Module is: 2.406 ms\n"
          ]
        }
      ],
      "source": [
        "def evaluate_timer(module, input_tvm, config, output_tvm):\n",
        "  eval = module.time_evaluator(module.entry_name, tvm.cpu(), number=10)\n",
        "  mean_time = eval(input_tvm, *config, output_tvm).mean\n",
        "  return mean_time\n",
        "\n",
        "original_module_et = evaluate_timer(module, input_tvm, config, output_tvm)\n",
        "print(\"Execution Time for Original Module is: %.3f ms\" % (original_module_et * 1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4phB6f_HQVre"
      },
      "source": [
        "## Optimizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLFnniCjSQW2"
      },
      "source": [
        "### Perform optimizations like `Reordering`, `Vectorization` and `Parallelization` on `Original_Module`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S7E4Po6UArAV",
        "outputId": "45dbb6cb-c50f-4957-8aeb-d5b541512a06"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">cnn_original</span>(Input: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Kernel1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Kernel2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias_conv_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias_conv_2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Weight1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">2048</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Weight2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">10</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_weight: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_bias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_mean: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_var: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_weight: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_bias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_mean: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_var: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), LinearBias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        PaddedInput <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">34</span>, <span style=\"color: #008000\">34</span>))\n",
              "        Conv <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        ConvBias <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        ReLU <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        BatchNorm <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        Pooled <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        PaddedInput_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">18</span>, <span style=\"color: #008000\">18</span>))\n",
              "        Conv_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        ConvBias_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        ReLU_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        BatchNorm_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        Pooled_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">8</span>))\n",
              "        Flattened <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2048</span>))\n",
              "        Linear <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>))\n",
              "        LinearReLU <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>))\n",
              "        Linear_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">34</span>, <span style=\"color: #008000\">34</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;PaddedInput&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Input[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(PaddedInput[v_n, v_c, v_h, v_w])\n",
              "                PaddedInput[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_h <span style=\"color: #008000; font-weight: bold\">and</span> v_h <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">33</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_w <span style=\"color: #008000; font-weight: bold\">and</span> v_w <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">33</span>, Input[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> p_0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">16</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> q_0, p_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">2</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">for</span> q_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">2</span>):\n",
              "                        <span style=\"color: #008000; font-weight: bold\">for</span> rc, ry, rx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
              "                            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Conv&quot;</span>):\n",
              "                                v_n, v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, k])\n",
              "                                v_p <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">32</span>, p_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> p_1)\n",
              "                                v_q <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">32</span>, q_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> q_1)\n",
              "                                v_rc, v_ry, v_rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;RRR&quot;</span>, [rc, ry, rx])\n",
              "                                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(PaddedInput[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx], Kernel1[v_k, v_rc, v_ry, v_rx])\n",
              "                                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Conv[v_n, v_k, v_p, v_q])\n",
              "                                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                                    Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                                Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> PaddedInput[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Kernel1[v_k, v_rc, v_ry, v_rx]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ConvBias&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, k, p, q])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Conv[v_n, v_k, v_p, v_q], Bias_conv_1[v_k])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ConvBias[v_n, v_k, v_p, v_q])\n",
              "                ConvBias[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias_conv_1[v_k]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ReLU&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ConvBias[v_n, v_c, v_h, v_w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ReLU[v_n, v_c, v_h, v_w])\n",
              "                ReLU[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(ConvBias[v_n, v_c, v_h, v_w], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;BatchNorm&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ReLU[v_n, v_c, v_h, v_w], BN1_mean[v_c], BN1_var[v_c], BN1_weight[v_c], BN1_bias[v_c])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(BatchNorm[v_n, v_c, v_h, v_w])\n",
              "                BatchNorm[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> (ReLU[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">-</span> BN1_mean[v_c]) <span style=\"color: #AA22FF; font-weight: bold\">/</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>sqrt(BN1_var[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">1.0000000000000001e-05</span>)) <span style=\"color: #AA22FF; font-weight: bold\">*</span> BN1_weight[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> BN1_bias[v_c]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w, rph, rpw <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Pooled&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w, v_rph, v_rpw <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRR&quot;</span>, [n, c, h, w, rph, rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(BatchNorm[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Pooled[v_n, v_c, v_h, v_w])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Pooled[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #AA22FF; font-weight: bold\">-</span><span style=\"color: #008000\">340282346638528859811704183484516925440.0</span>)\n",
              "                Pooled[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Pooled[v_n, v_c, v_h, v_w], BatchNorm[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">18</span>, <span style=\"color: #008000\">18</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;PaddedInput_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Pooled[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(PaddedInput_1[v_n, v_c, v_h, v_w])\n",
              "                PaddedInput_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_h <span style=\"color: #008000; font-weight: bold\">and</span> v_h <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">17</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_w <span style=\"color: #008000; font-weight: bold\">and</span> v_w <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">17</span>, Pooled[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q, rc, ry, rx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Conv_1&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q, v_rc, v_ry, v_rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [n, k, p, q, rc, ry, rx])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(PaddedInput_1[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx], Kernel2[v_k, v_rc, v_ry, v_rx])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Conv_1[v_n, v_k, v_p, v_q])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> PaddedInput_1[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Kernel2[v_k, v_rc, v_ry, v_rx]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ConvBias_1&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, k, p, q])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Conv_1[v_n, v_k, v_p, v_q], Bias_conv_2[v_k])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ConvBias_1[v_n, v_k, v_p, v_q])\n",
              "                ConvBias_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias_conv_2[v_k]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ReLU_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ConvBias_1[v_n, v_c, v_h, v_w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ReLU_1[v_n, v_c, v_h, v_w])\n",
              "                ReLU_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(ConvBias_1[v_n, v_c, v_h, v_w], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;BatchNorm_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ReLU_1[v_n, v_c, v_h, v_w], BN2_mean[v_c], BN2_var[v_c], BN2_weight[v_c], BN2_bias[v_c])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(BatchNorm_1[v_n, v_c, v_h, v_w])\n",
              "                BatchNorm_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> (ReLU_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">-</span> BN2_mean[v_c]) <span style=\"color: #AA22FF; font-weight: bold\">/</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>sqrt(BN2_var[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">1.0000000000000001e-05</span>)) <span style=\"color: #AA22FF; font-weight: bold\">*</span> BN2_weight[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> BN2_bias[v_c]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w, rph, rpw <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Pooled_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w, v_rph, v_rpw <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRR&quot;</span>, [n, c, h, w, rph, rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(BatchNorm_1[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Pooled_1[v_n, v_c, v_h, v_w])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Pooled_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #AA22FF; font-weight: bold\">-</span><span style=\"color: #008000\">340282346638528859811704183484516925440.0</span>)\n",
              "                Pooled_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Pooled_1[v_n, v_c, v_h, v_w], BatchNorm_1[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, i <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2048</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Flattened&quot;</span>):\n",
              "                v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Pooled_1[v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">64</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span>])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Flattened[v_n, v_i])\n",
              "                Flattened[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Pooled_1[v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">64</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span>]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o, r <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">2048</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Linear&quot;</span>):\n",
              "                v_n, v_o, v_r <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [n, o, r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Flattened[v_n, v_r], Weight1[v_o, v_r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Linear[v_n, v_o])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Flattened[v_n, v_r] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Weight1[v_o, v_r]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;LinearReLU&quot;</span>):\n",
              "                v_n, v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Linear[v_n, v_o], Bias1[v_o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(LinearReLU[v_n, v_o])\n",
              "                LinearReLU[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias1[v_o], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o, r <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Linear_1&quot;</span>):\n",
              "                v_n, v_o, v_r <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [n, o, r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(LinearReLU[v_n, v_r], Weight2[v_o, v_r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Linear_1[v_n, v_o])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> LinearReLU[v_n, v_r] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Weight2[v_o, v_r]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;LinearBias&quot;</span>):\n",
              "                v_n, v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Linear_1[v_n, v_o], Bias2[v_o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(LinearBias[v_n, v_o])\n",
              "                LinearBias[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias2[v_o]\n",
              "</pre></div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sch_manual_optim = tvm.tir.Schedule(Original_Module)\n",
        "sch_manual_optim.work_on(\"cnn_original\")\n",
        "block_conv = sch_manual_optim.get_block(\"Conv\")\n",
        "n, k, p, q, rc, ry, rx = sch_manual_optim.get_loops(block_conv)\n",
        "\n",
        "p_outer, p_inner = sch_manual_optim.split(p, factors=[16, 2])\n",
        "q_outer, q_inner = sch_manual_optim.split(q, factors=[16, 2])\n",
        "\n",
        "# Reordering\n",
        "sch_manual_optim.reorder(n, k, p_outer, q_outer, p_inner, q_inner, rc, ry, rx)\n",
        "\n",
        "# Vectorization\n",
        "sch_manual_optim.vectorize(q_inner)\n",
        "\n",
        "# Parallelization\n",
        "sch_manual_optim.parallel(p_outer)\n",
        "\n",
        "sch_manual_optim.mod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlykd2ugS139"
      },
      "source": [
        "### Manual Optimization Build & Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEFJKlQGTzKk"
      },
      "source": [
        "#### Build `manually_optimized_module`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW60YY62A2xB"
      },
      "outputs": [],
      "source": [
        "manual_optimized_module = tvm.build(sch_manual_optim.mod, [Input, Kernel1, Kernel2, Bias_conv1, Bias_conv2,\n",
        "                        Weight1, Bias1, Weight2, Bias2,\n",
        "                        BN1_weight, BN1_bias, BN1_mean, BN1_var,\n",
        "                        BN2_weight, BN2_bias, BN2_mean, BN2_var, fc2], target=\"llvm\", name=\"cnn_manual_optimized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KICL0JPYtGqz"
      },
      "source": [
        "#### Inference & Testing Correctness of Manual Optimization Module\n",
        " - Accuracy %\n",
        " - Prediction\n",
        " - Assertion Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTHBkSp6sazL",
        "outputId": "bb916776-b64f-497f-de33-039f9186eba6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Accuracy is : 79.17 %\n",
            "Manual Optimization Module's Prediction is: cat\n",
            "Assertion Test Passed for Manual Optimization.\n"
          ]
        }
      ],
      "source": [
        "output_tvm_manual = tvm.nd.empty((N, 10), device=ctx)\n",
        "input_tvm_manual = inference(manual_optimized_module, test_loader, config, output_tvm_manual)\n",
        "input_tvm_manual = tvm.nd.array(img, ctx)\n",
        "input_tvm_manual.shape\n",
        "manual_optimized_module(input_tvm_manual, *config, output_tvm_manual)\n",
        "pred_kind = np.argmax(output_tvm_manual.numpy(), axis=1)\n",
        "print(\"Manual Optimization Module's Prediction is:\", class_names[pred_kind[0]])\n",
        "try:\n",
        "    np.testing.assert_allclose(pred.numpy(), output_tvm_manual.asnumpy(), rtol=1e-4)\n",
        "    print(\"Assertion Test Passed for Manual Optimization.\")\n",
        "except AssertionError as e:\n",
        "    print(f\"AssertionError: {e}\")\n",
        "    print(\"Assertion Test Failed for Manual Optimization.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0-P5seAtmdR"
      },
      "source": [
        "#### Evaluate execution time of Manual Optimization Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOEVY_SJtsAv",
        "outputId": "5fe76ced-3ae2-4996-9436-bf989bfa8348"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time for Manual Optimization Module is : 2.245 ms\n"
          ]
        }
      ],
      "source": [
        "manual_optimized_module_et = evaluate_timer(manual_optimized_module, input_tvm, config, output_tvm)\n",
        "print(\"Execution Time for Manual Optimization Module is : %.3f ms\" % (manual_optimized_module_et * 1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSYnGfIrTJ8M"
      },
      "source": [
        "### Auto Optimization Build & Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "V11JnVCKBIIP",
        "outputId": "f4b167c9-fbe8-49ed-ff80-ed233b596d57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-10-04 00:16:05 [INFO] Logging directory: ./tune_tmp/logs\n",
            "2024-10-04 00:17:06 [INFO] LocalBuilder: max_workers = 1\n",
            "2024-10-04 00:17:08 [INFO] LocalRunner: max_workers = 1\n",
            "2024-10-04 00:17:10 [INFO] [task_scheduler.cc:159] Initializing Task #0: \"cnn_original\"\n",
            "2024-10-04 00:17:54 [INFO] [task_scheduler.cc:320] \n",
            " ID |         Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "  0 | cnn_original | 3967754 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Total trials: 0\n",
            "Total latency (us): 0\n",
            "\n",
            "2024-10-04 00:17:54 [INFO] [task_scheduler.cc:180] TaskScheduler picks Task #0: \"cnn_original\"\n",
            "2024-10-04 00:22:21 [INFO] [task_scheduler.cc:193] Sending 63 sample(s) to builder\n",
            "2024-10-04 00:28:11 [INFO] [task_scheduler.cc:195] Sending 63 sample(s) to runner\n",
            "2024-10-04 00:28:33 [DEBUG] XGB iter   0: tr-p-rmse: 9.483478\ttr-a-peak@32: 0.842171\ttr-rmse: 1.308913\ttr-rmse: 1.308913\n",
            "2024-10-04 00:28:34 [DEBUG] XGB iter  25: tr-p-rmse: 1395286.371601\ttr-a-peak@32: 0.826024\ttr-rmse: 106409.015989\ttr-rmse: 106409.015989\n",
            "2024-10-04 00:28:34 [DEBUG] XGB iter  50: tr-p-rmse: 203487978747.375031\ttr-a-peak@32: 0.842171\ttr-rmse: 15518729245.872486\ttr-rmse: 15518729245.872486\n",
            "2024-10-04 00:28:34 [DEBUG] XGB stopped. Best iteration: [0] tr-p-rmse:9.48348\ttr-a-peak@32:0.84217\ttr-rmse:1.30891\ttr-rmse:1.30891 \n",
            "2024-10-04 00:28:34 [INFO] [task_scheduler.cc:237] [Updated] Task #0: \"cnn_original\"\n",
            "2024-10-04 00:28:34 [INFO] [task_scheduler.cc:320] \n",
            " ID |         Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "  0 | cnn_original | 3967754 |      1 |         8.7494 |     453.4888 |              453.4888 |     63 |      \n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Total trials: 63\n",
            "Total latency (us): 453.489\n",
            "\n",
            "2024-10-04 00:28:34 [INFO] [task_scheduler.cc:180] TaskScheduler picks Task #0: \"cnn_original\"\n",
            "2024-10-04 00:32:55 [INFO] [task_scheduler.cc:193] Sending 1 sample(s) to builder\n",
            "2024-10-04 00:32:58 [INFO] [task_scheduler.cc:195] Sending 1 sample(s) to runner\n",
            "2024-10-04 00:32:59 [DEBUG] XGB validation: p-rmse: 217262209024.507843\ta-peak@32: 1.000000\n",
            "2024-10-04 00:32:59 [INFO] [task_scheduler.cc:237] [Updated] Task #0: \"cnn_original\"\n",
            "2024-10-04 00:32:59 [INFO] [task_scheduler.cc:320] \n",
            " ID |         Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "  0 | cnn_original | 3967754 |      1 |         8.7494 |     453.4888 |              453.4888 |     64 |      \n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Total trials: 64\n",
            "Total latency (us): 453.489\n",
            "\n",
            "2024-10-04 00:32:59 [INFO] [task_scheduler.cc:260] Task #0 has finished. Remaining task(s): 0\n",
            "2024-10-04 00:32:59 [INFO] [task_scheduler.cc:320] \n",
            " ID |         Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "  0 | cnn_original | 3967754 |      1 |         8.7494 |     453.4888 |              453.4888 |     64 |    Y \n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Total trials: 64\n",
            "Total latency (us): 453.489\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(Input: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Kernel1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Kernel2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias_conv_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias_conv_2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Weight1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">2048</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Weight2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">10</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_weight: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_bias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_mean: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_var: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_weight: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_bias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_mean: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_var: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), LinearBias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;cnn_original&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        PaddedInput <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">34</span>, <span style=\"color: #008000\">34</span>))\n",
              "        Conv <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        Pooled <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        PaddedInput_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">18</span>, <span style=\"color: #008000\">18</span>))\n",
              "        Conv_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        Pooled_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">8</span>))\n",
              "        Flattened <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2048</span>))\n",
              "        Linear <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>))\n",
              "        Linear_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>))\n",
              "        Linear_rf <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">2</span>))\n",
              "        Pooled_global <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">8</span>))\n",
              "        Conv_global <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n_0_k_0_p_0_q_0_fused_fused <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">4</span>, annotations<span style=\"color: #AA22FF; font-weight: bold\">=</span>{<span style=\"color: #BA2121\">&quot;pragma_auto_unroll_max_step&quot;</span>: <span style=\"color: #008000\">64</span>, <span style=\"color: #BA2121\">&quot;pragma_unroll_explicit&quot;</span>: <span style=\"color: #008000\">1</span>}):\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1, ax2 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">34</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> ax3_fused <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">10</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;PaddedInput&quot;</span>):\n",
              "                        v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSS&quot;</span>, [ax0, ax1, ax2])\n",
              "                        v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">34</span>, n_0_k_0_p_0_q_0_fused_fused <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax3_fused)\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Input[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(PaddedInput[v_n, v_c, v_h, v_w])\n",
              "                        PaddedInput[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_h <span style=\"color: #008000; font-weight: bold\">and</span> v_h <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">33</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_w <span style=\"color: #008000; font-weight: bold\">and</span> v_w <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">33</span>, Input[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> n_1, k_1, p_1, q_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">2</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> n_2_init, k_2_init, p_2_init, q_2_init, n_3_init, k_3_init, p_3_init <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">1</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">for</span> q_3_fused_init <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">2</span>):\n",
              "                        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Conv_init&quot;</span>):\n",
              "                            v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, n_1 <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_2_init <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_3_init)\n",
              "                            v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, k_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_2_init <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_3_init)\n",
              "                            v_p <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">32</span>, p_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> p_2_init <span style=\"color: #AA22FF; font-weight: bold\">+</span> p_3_init)\n",
              "                            v_q <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">32</span>, n_0_k_0_p_0_q_0_fused_fused <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> q_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> q_2_init <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> q_3_fused_init)\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads()\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Conv[v_n, v_k, v_p, v_q])\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block_attr({<span style=\"color: #BA2121\">&quot;meta_schedule.tiling_structure&quot;</span>: <span style=\"color: #BA2121\">&quot;SSRSRS&quot;</span>})\n",
              "                            Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> rc_0, ry_0, rx_0, n_2, k_2, p_2, q_2, rc_1, ry_1, rx_1, n_3, k_3, p_3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">1</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">for</span> q_3_fused <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">2</span>):\n",
              "                        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Conv_update&quot;</span>):\n",
              "                            v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, n_1 <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_3)\n",
              "                            v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, k_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_2 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_3)\n",
              "                            v_p <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">32</span>, p_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> p_2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> p_3)\n",
              "                            v_q <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">32</span>, n_0_k_0_p_0_q_0_fused_fused <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> q_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> q_2 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> q_3_fused)\n",
              "                            v_rc <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">3</span>, rc_0 <span style=\"color: #AA22FF; font-weight: bold\">+</span> rc_1)\n",
              "                            v_ry <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">3</span>, ry_0 <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry_1)\n",
              "                            v_rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">3</span>, rx_0 <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx_1)\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Conv[v_n, v_k, v_p, v_q], PaddedInput[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx], Kernel1[v_k, v_rc, v_ry, v_rx])\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Conv[v_n, v_k, v_p, v_q])\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block_attr({<span style=\"color: #BA2121\">&quot;meta_schedule.tiling_structure&quot;</span>: <span style=\"color: #BA2121\">&quot;SSRSRS&quot;</span>})\n",
              "                            Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> PaddedInput[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Kernel1[v_k, v_rc, v_ry, v_rx]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n_0_c_0_h_0_w_0_n_1_c_1_fused <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">32</span>, annotations<span style=\"color: #AA22FF; font-weight: bold\">=</span>{<span style=\"color: #BA2121\">&quot;pragma_auto_unroll_max_step&quot;</span>: <span style=\"color: #008000\">64</span>, <span style=\"color: #BA2121\">&quot;pragma_unroll_explicit&quot;</span>: <span style=\"color: #008000\">1</span>}):\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> h_1, w_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">1</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> n_2_init, c_2_init, h_2_init, w_2_init, n_3_init, c_3_init, h_3_init <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">for</span> w_3_fused_init <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">4</span>):\n",
              "                        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Pooled_init&quot;</span>):\n",
              "                            v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, n_2_init <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_3_init)\n",
              "                            v_c <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, n_0_c_0_h_0_w_0_n_1_c_1_fused <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> c_2_init <span style=\"color: #AA22FF; font-weight: bold\">+</span> c_3_init)\n",
              "                            v_h <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, n_0_c_0_h_0_w_0_n_1_c_1_fused <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> h_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> h_2_init <span style=\"color: #AA22FF; font-weight: bold\">+</span> h_3_init)\n",
              "                            v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, w_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">16</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> w_2_init <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> w_3_fused_init)\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads()\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Pooled[v_n, v_c, v_h, v_w])\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block_attr({<span style=\"color: #BA2121\">&quot;meta_schedule.tiling_structure&quot;</span>: <span style=\"color: #BA2121\">&quot;SSRSRS&quot;</span>})\n",
              "                            Pooled[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #AA22FF; font-weight: bold\">-</span><span style=\"color: #008000\">340282346638528859811704183484516925440.0</span>)\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> rph_0, rpw_0, n_2, c_2, h_2, w_2, rph_1, rpw_1, n_3, c_3, h_3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">for</span> w_3_fused <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">4</span>):\n",
              "                        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Pooled_update&quot;</span>):\n",
              "                            v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, n_2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_3)\n",
              "                            v_c <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, n_0_c_0_h_0_w_0_n_1_c_1_fused <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> c_2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> c_3)\n",
              "                            v_h <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, n_0_c_0_h_0_w_0_n_1_c_1_fused <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> h_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> h_2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> h_3)\n",
              "                            v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, w_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">16</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> w_2 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> w_3_fused)\n",
              "                            v_rph <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">2</span>, rph_0 <span style=\"color: #AA22FF; font-weight: bold\">+</span> rph_1)\n",
              "                            v_rpw <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">2</span>, rpw_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> rpw_1)\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Pooled[v_n, v_c, v_h, v_w], Conv[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw], Bias_conv_1[v_c], BN1_mean[v_c], BN1_var[v_c], BN1_weight[v_c], BN1_bias[v_c])\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Pooled[v_n, v_c, v_h, v_w])\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block_attr({<span style=\"color: #BA2121\">&quot;meta_schedule.tiling_structure&quot;</span>: <span style=\"color: #BA2121\">&quot;SSRSRS&quot;</span>})\n",
              "                            Pooled[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Pooled[v_n, v_c, v_h, v_w], (T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Conv[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias_conv_1[v_c], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-</span> BN1_mean[v_c]) <span style=\"color: #AA22FF; font-weight: bold\">/</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>sqrt(BN1_var[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">1.0000000000000001e-05</span>)) <span style=\"color: #AA22FF; font-weight: bold\">*</span> BN1_weight[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> BN1_bias[v_c])\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1, ax2 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">for</span> ax3_fused <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">16</span>):\n",
              "                        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;PaddedInput_1&quot;</span>):\n",
              "                            v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, ax0)\n",
              "                            v_c <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, n_0_c_0_h_0_w_0_n_1_c_1_fused <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax1)\n",
              "                            v_h <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">18</span>, n_0_c_0_h_0_w_0_n_1_c_1_fused <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> h_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">1</span>)\n",
              "                            v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">18</span>, ax3_fused <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">1</span>)\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Pooled[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(PaddedInput_1[v_n, v_c, v_h, v_w])\n",
              "                            PaddedInput_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_h <span style=\"color: #008000; font-weight: bold\">and</span> v_h <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">17</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_w <span style=\"color: #008000; font-weight: bold\">and</span> v_w <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">17</span>, Pooled[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n_0_k_0_p_0_q_0_fused <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">32</span>, annotations<span style=\"color: #AA22FF; font-weight: bold\">=</span>{<span style=\"color: #BA2121\">&quot;pragma_auto_unroll_max_step&quot;</span>: <span style=\"color: #008000\">64</span>, <span style=\"color: #BA2121\">&quot;pragma_unroll_explicit&quot;</span>: <span style=\"color: #008000\">1</span>}):\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> n_1, k_1, p_1, q_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> n_2_init, k_2_init, p_2_init, q_2_init, n_3_init, k_3_init, p_3_init, q_3_init <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">1</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Conv_1_init&quot;</span>):\n",
              "                        v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, n_1 <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_2_init <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_3_init)\n",
              "                        v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">32</span>, n_0_k_0_p_0_q_0_fused <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_2_init <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_3_init)\n",
              "                        v_p <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, n_0_k_0_p_0_q_0_fused <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> p_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> p_2_init <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> p_3_init)\n",
              "                        v_q <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, n_0_k_0_p_0_q_0_fused <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> q_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> q_2_init <span style=\"color: #AA22FF; font-weight: bold\">+</span> q_3_init)\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads()\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Conv_global[v_n, v_k, v_p, v_q])\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block_attr({<span style=\"color: #BA2121\">&quot;meta_schedule.tiling_structure&quot;</span>: <span style=\"color: #BA2121\">&quot;SSRSRS&quot;</span>})\n",
              "                        Conv_global[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> rc_0, ry_0, rx_0, n_2, k_2, p_2, q_2, rc_1, ry_1, rx_1, n_3, k_3, p_3, q_3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">1</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Conv_1_update&quot;</span>):\n",
              "                        v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, n_1 <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_3)\n",
              "                        v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">32</span>, n_0_k_0_p_0_q_0_fused <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_2 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_3)\n",
              "                        v_p <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, n_0_k_0_p_0_q_0_fused <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> p_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> p_2 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> p_3)\n",
              "                        v_q <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, n_0_k_0_p_0_q_0_fused <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> q_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> q_2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> q_3)\n",
              "                        v_rc <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">16</span>, rc_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">16</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> rc_1)\n",
              "                        v_ry <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">3</span>, ry_0 <span style=\"color: #AA22FF; font-weight: bold\">+</span> ry_1)\n",
              "                        v_rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">3</span>, rx_0 <span style=\"color: #AA22FF; font-weight: bold\">+</span> rx_1)\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Conv_global[v_n, v_k, v_p, v_q], PaddedInput_1[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx], Kernel2[v_k, v_rc, v_ry, v_rx])\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Conv_global[v_n, v_k, v_p, v_q])\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block_attr({<span style=\"color: #BA2121\">&quot;meta_schedule.tiling_structure&quot;</span>: <span style=\"color: #BA2121\">&quot;SSRSRS&quot;</span>})\n",
              "                        Conv_global[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv_global[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> PaddedInput_1[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Kernel2[v_k, v_rc, v_ry, v_rx]\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1, ax2 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">4</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> ax3_fused <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">8</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Conv_global&quot;</span>):\n",
              "                        v0 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, ax0)\n",
              "                        v1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">32</span>, n_0_k_0_p_0_q_0_fused <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax1)\n",
              "                        v2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, n_0_k_0_p_0_q_0_fused <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax2)\n",
              "                        v3 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, n_0_k_0_p_0_q_0_fused <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax3_fused)\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Conv_global[v0, v1, v2, v3])\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Conv_1[v0, v1, v2, v3])\n",
              "                        Conv_1[v0, v1, v2, v3] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv_global[v0, v1, v2, v3]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n_0_c_0_h_0_w_0_fused <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">2</span>, annotations<span style=\"color: #AA22FF; font-weight: bold\">=</span>{<span style=\"color: #BA2121\">&quot;pragma_auto_unroll_max_step&quot;</span>: <span style=\"color: #008000\">64</span>, <span style=\"color: #BA2121\">&quot;pragma_unroll_explicit&quot;</span>: <span style=\"color: #008000\">1</span>}):\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> n_1, c_1, h_1, w_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">1</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> n_2_init, c_2_init, h_2_init, w_2_init, n_3_init, c_3_init, h_3_init <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">1</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">for</span> w_3_fused_init <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">2</span>):\n",
              "                        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Pooled_1_init&quot;</span>):\n",
              "                            v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, n_1 <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_2_init <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_3_init)\n",
              "                            v_c <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">32</span>, c_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">16</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> c_2_init <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> c_3_init)\n",
              "                            v_h <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">8</span>, n_0_c_0_h_0_w_0_fused <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> h_1 <span style=\"color: #AA22FF; font-weight: bold\">+</span> h_2_init <span style=\"color: #AA22FF; font-weight: bold\">+</span> h_3_init)\n",
              "                            v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">8</span>, w_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> w_2_init <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> w_3_fused_init)\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads()\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Pooled_global[v_n, v_c, v_h, v_w])\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block_attr({<span style=\"color: #BA2121\">&quot;meta_schedule.tiling_structure&quot;</span>: <span style=\"color: #BA2121\">&quot;SSRSRS&quot;</span>})\n",
              "                            Pooled_global[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #AA22FF; font-weight: bold\">-</span><span style=\"color: #008000\">340282346638528859811704183484516925440.0</span>)\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> rph_0, rpw_0, n_2, c_2, h_2, w_2, rph_1, rpw_1, n_3, c_3, h_3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">1</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">for</span> w_3_fused <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">2</span>):\n",
              "                        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Pooled_1_update&quot;</span>):\n",
              "                            v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, n_1 <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_3)\n",
              "                            v_c <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">32</span>, c_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">16</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> c_2 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> c_3)\n",
              "                            v_h <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">8</span>, n_0_c_0_h_0_w_0_fused <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> h_1 <span style=\"color: #AA22FF; font-weight: bold\">+</span> h_2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> h_3)\n",
              "                            v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">8</span>, w_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> w_2 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> w_3_fused)\n",
              "                            v_rph <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">2</span>, rph_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> rph_1)\n",
              "                            v_rpw <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">2</span>, rpw_0 <span style=\"color: #AA22FF; font-weight: bold\">+</span> rpw_1)\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Pooled_global[v_n, v_c, v_h, v_w], Conv_1[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw], Bias_conv_2[v_c], BN2_mean[v_c], BN2_var[v_c], BN2_weight[v_c], BN2_bias[v_c])\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Pooled_global[v_n, v_c, v_h, v_w])\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block_attr({<span style=\"color: #BA2121\">&quot;meta_schedule.tiling_structure&quot;</span>: <span style=\"color: #BA2121\">&quot;SSRSRS&quot;</span>})\n",
              "                            Pooled_global[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Pooled_global[v_n, v_c, v_h, v_w], (T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Conv_1[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias_conv_2[v_c], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-</span> BN2_mean[v_c]) <span style=\"color: #AA22FF; font-weight: bold\">/</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>sqrt(BN2_var[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">1.0000000000000001e-05</span>)) <span style=\"color: #AA22FF; font-weight: bold\">*</span> BN2_weight[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> BN2_bias[v_c])\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> ax2_ax3_fused <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">32</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Pooled_global&quot;</span>):\n",
              "                        v0, v1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
              "                        v2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">8</span>, n_0_c_0_h_0_w_0_fused <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax2_ax3_fused <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span>)\n",
              "                        v3 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">8</span>, ax2_ax3_fused <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span>)\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Pooled_global[v0, v1, v2, v3])\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Pooled_1[v0, v1, v2, v3])\n",
              "                        Pooled_1[v0, v1, v2, v3] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Pooled_global[v0, v1, v2, v3]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n_0_o_0_fused_fused <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">8</span>, annotations<span style=\"color: #AA22FF; font-weight: bold\">=</span>{<span style=\"color: #BA2121\">&quot;pragma_auto_unroll_max_step&quot;</span>: <span style=\"color: #008000\">64</span>, <span style=\"color: #BA2121\">&quot;pragma_unroll_explicit&quot;</span>: <span style=\"color: #008000\">1</span>}):\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2048</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Flattened&quot;</span>):\n",
              "                    v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Pooled_1[v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">64</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span>])\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Flattened[v_n, v_i])\n",
              "                    Flattened[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Pooled_1[v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">64</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span>]\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> n_1, o_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">4</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> n_2_init, o_2_init, n_3_init, o_3_init <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Linear_init&quot;</span>):\n",
              "                        v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, n_1 <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_2_init <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_3_init)\n",
              "                        v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">128</span>, n_0_o_0_fused_fused <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">16</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> o_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> o_2_init <span style=\"color: #AA22FF; font-weight: bold\">+</span> o_3_init)\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads()\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Linear[v_n, v_o])\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block_attr({<span style=\"color: #BA2121\">&quot;meta_schedule.tiling_structure&quot;</span>: <span style=\"color: #BA2121\">&quot;SSRSRS&quot;</span>})\n",
              "                        Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> r_0, n_2, o_2, r_1, n_3, o_3 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Linear_update&quot;</span>):\n",
              "                        v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, n_1 <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_3)\n",
              "                        v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">128</span>, n_0_o_0_fused_fused <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">16</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> o_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> o_2 <span style=\"color: #AA22FF; font-weight: bold\">+</span> o_3)\n",
              "                        v_r <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">2048</span>, r_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> r_1)\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Linear[v_n, v_o], Flattened[v_n, v_r], Weight1[v_o, v_r])\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Linear[v_n, v_o])\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block_attr({<span style=\"color: #BA2121\">&quot;meta_schedule.tiling_structure&quot;</span>: <span style=\"color: #BA2121\">&quot;SSRSRS&quot;</span>})\n",
              "                        Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Flattened[v_n, v_r] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Weight1[v_o, v_r]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n_o_fused <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">10</span>, annotations<span style=\"color: #AA22FF; font-weight: bold\">=</span>{<span style=\"color: #BA2121\">&quot;pragma_auto_unroll_max_step&quot;</span>: <span style=\"color: #008000\">64</span>, <span style=\"color: #BA2121\">&quot;pragma_unroll_explicit&quot;</span>: <span style=\"color: #008000\">1</span>}):\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1, ax2 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Linear_1_rf_init&quot;</span>):\n",
              "                    vr_1, v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
              "                    v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">10</span>, n_o_fused <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax2)\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads()\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Linear_rf[v_n, v_o, vr_1])\n",
              "                    Linear_rf[v_n, v_o, vr_1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> ax3 <span style=\"color: #008000; font-weight: bold\">in</span> range(<span style=\"color: #008000\">64</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Linear_1_rf_update&quot;</span>):\n",
              "                        vr_1, v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
              "                        v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">10</span>, n_o_fused <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax2)\n",
              "                        vr_0 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">64</span>, ax3)\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Linear_rf[v_n, v_o, vr_1], Linear[v_n, vr_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> vr_1], Bias1[vr_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> vr_1], Weight2[v_o, vr_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> vr_1])\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Linear_rf[v_n, v_o, vr_1])\n",
              "                        Linear_rf[v_n, v_o, vr_1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear_rf[v_n, v_o, vr_1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Linear[v_n, vr_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> vr_1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias1[vr_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> vr_1], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)) <span style=\"color: #AA22FF; font-weight: bold\">*</span> Weight2[v_o, vr_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> vr_1]\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> ax1_init, ax2_init <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Linear_1_init&quot;</span>):\n",
              "                    v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, ax1_init)\n",
              "                    v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">10</span>, n_o_fused <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax2_init)\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads()\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Linear_1[v_n, v_o])\n",
              "                    Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1, ax2 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Linear_1_update&quot;</span>):\n",
              "                    vr_1, v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;RS&quot;</span>, [ax0, ax1])\n",
              "                    v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">10</span>, n_o_fused <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax2)\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Linear_1[v_n, v_o], Linear_rf[v_n, v_o, vr_1])\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Linear_1[v_n, v_o])\n",
              "                    Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Linear_rf[v_n, v_o, vr_1]\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;LinearBias&quot;</span>):\n",
              "                v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">0</span>)\n",
              "                v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">10</span>, n_o_fused)\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Linear_1[v_n, v_o], Bias2[v_o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(LinearBias[v_n, v_o])\n",
              "                LinearBias[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias2[v_o]\n",
              "</pre></div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tvm import meta_schedule as ms\n",
        "\n",
        "database = ms.tune_tir(\n",
        "    mod=Original_Module,\n",
        "    target=\"llvm --num-cores=1\",\n",
        "    max_trials_global=64,\n",
        "    num_trials_per_iter=64,\n",
        "    work_dir=\"./tune_tmp\",\n",
        ")\n",
        "\n",
        "sch_tuned = ms.tir_integration.compile_tir(database, Original_Module, target=\"llvm --num-cores=1\")\n",
        "sch_tuned.mod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5c4H614UH_t"
      },
      "source": [
        "#### Build `auto_optimized_module`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yUcjAOZkBWiM"
      },
      "outputs": [],
      "source": [
        "auto_optimized_module = tvm.build(sch_tuned.mod, [Input, Kernel1, Kernel2, Bias_conv1, Bias_conv2,\n",
        "                        Weight1, Bias1, Weight2, Bias2,\n",
        "                        BN1_weight, BN1_bias, BN1_mean, BN1_var,\n",
        "                        BN2_weight, BN2_bias, BN2_mean, BN2_var, fc2], target=\"llvm\", name=\"cnn_auto_optimized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMEoKOHTuGjM"
      },
      "source": [
        "#### Inference & Testing Correctness of Auto Optimization Module\n",
        " - Accuracy %\n",
        " - Prediction\n",
        " - Assertion Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fFmWQwZAuHjy",
        "outputId": "e1167a11-db09-4f52-8c3f-bca15e951756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Accuracy is : 78.44 %\n",
            "Auto Optimization Module's  Prediction is: cat\n",
            "Assertion Test Passed for Auto Optimization.\n"
          ]
        }
      ],
      "source": [
        "output_tvm_auto = tvm.nd.empty((N, 10), device=ctx)\n",
        "input_tvm_auto = inference(auto_optimized_module, test_loader, config, output_tvm_auto)\n",
        "input_tvm_auto = tvm.nd.array(img, ctx)\n",
        "input_tvm_auto.shape\n",
        "auto_optimized_module(input_tvm_auto, *config, output_tvm_auto)\n",
        "pred_kind = np.argmax(output_tvm_auto.numpy(), axis=1)\n",
        "print(\"Auto Optimization Module's  Prediction is:\", class_names[pred_kind[0]])\n",
        "try:\n",
        "    np.testing.assert_allclose(pred.numpy(), output_tvm_auto.asnumpy(), rtol=1e-1, atol=0.5)\n",
        "    print(\"Assertion Test Passed for Auto Optimization.\")\n",
        "except AssertionError as e:\n",
        "    print(f\"AssertionError: {e}\")\n",
        "    print(\"Assertion Test Failed for Auto Optimization.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNRgw2MZuMQ1"
      },
      "source": [
        "#### Evaluate execution time of Auto Optimization Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q0UmxBm8uTS4",
        "outputId": "bb140795-e56c-4681-ed84-f5b20f293ce8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time for Auto Optimization Module is: 0.680 ms\n"
          ]
        }
      ],
      "source": [
        "auto_optimized_module_et = evaluate_timer(auto_optimized_module, input_tvm, config, output_tvm)\n",
        "print(\"Execution Time for Auto Optimization Module is: %.3f ms\" % (auto_optimized_module_et * 1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aLrPUTGTRUq"
      },
      "source": [
        "################################################################################\n",
        "# 2. GEMM Method Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNz9SWUSgCDG"
      },
      "source": [
        "## Tensor IR Implemetation for the GEMM method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrqD6G1Lgn0i"
      },
      "source": [
        "Define the GEMM-CNN function containing the flow of the operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ANi4rdPxBQE4"
      },
      "outputs": [],
      "source": [
        "def gemm_original(Input, Kernels, Bias_conv, BNWeight, BNBias, BNMean, BNVariance, Weights, Bias, num_classes=10):\n",
        "\n",
        "    def Conv2DGEMM(Input, Kernel, Bias, stride=1, padding=1):\n",
        "        N, C, H, W = Input.shape\n",
        "        K, _, r, S = Kernel.shape\n",
        "        P = (H - r + 2 * padding) // stride + 1\n",
        "        Q = (W - S + 2 * padding) // stride + 1\n",
        "        padded_H = H + 2 * padding\n",
        "        padded_W = W + 2 * padding\n",
        "\n",
        "        padded_input = te.compute(\n",
        "            (N, C, padded_H, padded_W),\n",
        "            lambda n, c, h, w: te.if_then_else(\n",
        "                te.all(h >= padding, h < H + padding, w >= padding, w < W + padding),\n",
        "                Input[n, c, h - padding, w - padding],\n",
        "                tvm.tir.const(0.0, \"float32\")\n",
        "            ),\n",
        "            name=\"padded_input\"\n",
        "        )\n",
        "        # Converts patches of images into columns\n",
        "        im2col_result = te.compute(\n",
        "            (N, P * Q, C * r * S),\n",
        "            lambda n, pq, crs: padded_input[\n",
        "                n,\n",
        "                crs // (r * S),\n",
        "                (pq // Q) * stride + (crs % (r * S)) // S,\n",
        "                (pq % Q) * stride + (crs % S)\n",
        "            ],\n",
        "            name=\"im2col\"\n",
        "        )\n",
        "        # Flatten the kernel into columns\n",
        "        kernel2col_result = te.compute(\n",
        "            (K, C * r * S),\n",
        "            lambda k, crs: Kernel[\n",
        "                k,\n",
        "                crs // (r * S),\n",
        "                (crs % (r * S)) // S,\n",
        "                crs % S\n",
        "            ],\n",
        "            name=\"kernel2col\"\n",
        "        )\n",
        "        # Matrix Multiplication\n",
        "        k = te.reduce_axis((0, C * r * S), name=\"k\")\n",
        "        conv_result = te.compute(\n",
        "            (N, P * Q, K),\n",
        "            lambda n, pq, k_out: te.sum(\n",
        "                im2col_result[n, pq, k] * kernel2col_result[k_out, k],\n",
        "                axis=k\n",
        "            ),\n",
        "            name=\"conv\"\n",
        "        )\n",
        "        # Reshape into convolution output\n",
        "        reshaped_conv = te.compute(\n",
        "            (N, K, P, Q),\n",
        "            lambda n, k, p, q: conv_result[n, p * Q + q, k],\n",
        "            name=\"reshaped_conv\"\n",
        "        )\n",
        "\n",
        "        ConvBias = te.compute(\n",
        "            (N, K, P, Q),\n",
        "            lambda n, k, p, q: reshaped_conv[n, k, p, q] + Bias[k],\n",
        "            name=\"ConvBias\"\n",
        "        )\n",
        "\n",
        "        return ConvBias\n",
        "\n",
        "    def ReLU(Input):\n",
        "        N, C, H, W = Input.shape\n",
        "\n",
        "        return te.compute(\n",
        "            (N, C, H, W),\n",
        "            lambda n, c, h, w: te.max(Input[n, c, h, w], tvm.tir.const(0, \"float32\")),\n",
        "            name=\"ReLU\"\n",
        "        )\n",
        "\n",
        "    def BatchNormalization(Input, BNWeight, BNBias, BNMean, BNVariance, epsilon=1e-5):\n",
        "        N, C, H, W = Input.shape\n",
        "\n",
        "        normalized = te.compute(\n",
        "            (N, C, H, W),\n",
        "            lambda n, c, h, w: (Input[n, c, h, w] - BNMean[c]) / te.sqrt(BNVariance[c] + epsilon),\n",
        "            name=\"normalized\"\n",
        "        )\n",
        "\n",
        "        batchnorm_output = te.compute(\n",
        "            (N, C, H, W),\n",
        "            lambda n, c, h, w: normalized[n, c, h, w] * BNWeight[c] + BNBias[c],\n",
        "            name=\"batchnorm_output\"\n",
        "        )\n",
        "\n",
        "        return batchnorm_output\n",
        "\n",
        "    def MaxPool2D(Input, kernel_size=2):\n",
        "        N, C, H, W = Input.shape\n",
        "        P = H // kernel_size\n",
        "        Q = W // kernel_size\n",
        "\n",
        "        rph = te.reduce_axis((0, kernel_size), name=\"rph\")\n",
        "        rpw = te.reduce_axis((0, kernel_size), name=\"rpw\")\n",
        "\n",
        "        Pooled = te.compute(\n",
        "            (N, C, P, Q),\n",
        "            lambda n, c, h, w: te.max(\n",
        "                Input[n, c, h * kernel_size + rph, w * kernel_size + rpw],\n",
        "                axis=[rph, rpw]\n",
        "            ),\n",
        "            name=\"Pooled\"\n",
        "        )\n",
        "\n",
        "        return Pooled\n",
        "\n",
        "    def Flatten(Input):\n",
        "        N, C, H, W = Input.shape\n",
        "        P = C * H * W\n",
        "\n",
        "        Flattened = te.compute(\n",
        "            (N, P),\n",
        "            lambda n, i: Input[n, i // (H * W), (i // W) % H, i % W],\n",
        "            name=\"Flattened\"\n",
        "        )\n",
        "\n",
        "        return Flattened\n",
        "\n",
        "    def LinearReLU(Input, Weight, Bias):\n",
        "      N, I = Input.shape\n",
        "      O, _ = Weight.shape\n",
        "\n",
        "      r = te.reduce_axis((0, I), name=\"r\")\n",
        "\n",
        "      Linear = te.compute(\n",
        "          (N, O),\n",
        "          lambda n, o: te.sum(Input[n, r] * Weight[o, r], axis=r),\n",
        "          name=\"Linear\"\n",
        "      )\n",
        "\n",
        "      LinearBias = te.compute(\n",
        "          (N, O),\n",
        "          lambda n, o: Linear[n, o] + Bias[o],\n",
        "          name=\"LinearBias\"\n",
        "      )\n",
        "\n",
        "      LinearReLU = te.compute(\n",
        "          (N, O),\n",
        "          lambda n, o: te.max(LinearBias[n, o], tvm.tir.const(0, \"float32\")),\n",
        "          name=\"LinearReLU\"\n",
        "      )\n",
        "\n",
        "      return LinearReLU\n",
        "\n",
        "    def Linear(Input, Weight, Bias):\n",
        "      N, I = Input.shape\n",
        "      O, _ = Weight.shape\n",
        "\n",
        "      r = te.reduce_axis((0, I), name=\"r\")\n",
        "\n",
        "      Linear = te.compute(\n",
        "          (N, O),\n",
        "          lambda n, o: te.sum(Input[n, r] * Weight[o, r], axis=r),\n",
        "          name=\"Linear\"\n",
        "      )\n",
        "\n",
        "      LinearBias = te.compute(\n",
        "          (N, O),\n",
        "          lambda n, o: Linear[n, o] + Bias[o],\n",
        "          name=\"LinearBias\"\n",
        "      )\n",
        "\n",
        "      return LinearBias\n",
        "\n",
        "    # Forward pass\n",
        "    conv_1 = Conv2DGEMM(Input, Kernels[0], Bias_conv[0])\n",
        "    relu_1 = ReLU(conv_1)\n",
        "    bn_1 = BatchNormalization(relu_1, BNWeight[0], BNBias[0], BNMean[0], BNVariance[0])\n",
        "    maxpool2d_1 = MaxPool2D(bn_1)\n",
        "\n",
        "    conv_2 = Conv2DGEMM(maxpool2d_1, Kernels[1], Bias_conv[1])\n",
        "    relu_2 = ReLU(conv_2)\n",
        "    bn_2 = BatchNormalization(relu_2, BNWeight[1], BNBias[1], BNMean[1], BNVariance[1])\n",
        "    maxpool2d_2 = MaxPool2D(bn_2)\n",
        "\n",
        "    flatten = Flatten(maxpool2d_2)\n",
        "\n",
        "    fc1 = LinearReLU(flatten, Weights[0], Bias[0])\n",
        "    fc2 = Linear(fc1, Weights[1], Bias[1])\n",
        "\n",
        "    s = te.create_schedule(fc2.op)\n",
        "    return s, conv_1, conv_2, fc1, fc2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRPPfJvRhczu"
      },
      "source": [
        "Create TIR & Show the IRModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VS2fkpqzgVmP",
        "outputId": "2a86caef-9730-427a-c9cc-bee7d1f404f8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">gemm_original</span>(Input: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Kernel1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Kernel2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias_conv_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias_conv_2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Weight1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">2048</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Weight2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">10</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_weight: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_bias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_mean: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_var: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_weight: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_bias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_mean: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_var: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), LinearBias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        PaddedInput <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">34</span>, <span style=\"color: #008000\">34</span>))\n",
              "        Conv <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        ConvBias <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        ReLU <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        BatchNorm <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        Pooled <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        PaddedInput_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">18</span>, <span style=\"color: #008000\">18</span>))\n",
              "        Conv_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        ConvBias_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        ReLU_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        BatchNorm_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        Pooled_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">8</span>))\n",
              "        Flattened <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2048</span>))\n",
              "        Linear <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>))\n",
              "        LinearReLU <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>))\n",
              "        Linear_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">34</span>, <span style=\"color: #008000\">34</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;PaddedInput&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Input[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(PaddedInput[v_n, v_c, v_h, v_w])\n",
              "                PaddedInput[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_h <span style=\"color: #008000; font-weight: bold\">and</span> v_h <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">33</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_w <span style=\"color: #008000; font-weight: bold\">and</span> v_w <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">33</span>, Input[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q, rc, ry, rx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Conv&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q, v_rc, v_ry, v_rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [n, k, p, q, rc, ry, rx])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(PaddedInput[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx], Kernel1[v_k, v_rc, v_ry, v_rx])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Conv[v_n, v_k, v_p, v_q])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> PaddedInput[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Kernel1[v_k, v_rc, v_ry, v_rx]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ConvBias&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, k, p, q])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Conv[v_n, v_k, v_p, v_q], Bias_conv_1[v_k])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ConvBias[v_n, v_k, v_p, v_q])\n",
              "                ConvBias[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias_conv_1[v_k]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ReLU&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ConvBias[v_n, v_c, v_h, v_w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ReLU[v_n, v_c, v_h, v_w])\n",
              "                ReLU[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(ConvBias[v_n, v_c, v_h, v_w], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;BatchNorm&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ReLU[v_n, v_c, v_h, v_w], BN1_mean[v_c], BN1_var[v_c], BN1_weight[v_c], BN1_bias[v_c])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(BatchNorm[v_n, v_c, v_h, v_w])\n",
              "                BatchNorm[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> (ReLU[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">-</span> BN1_mean[v_c]) <span style=\"color: #AA22FF; font-weight: bold\">/</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>sqrt(BN1_var[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">1.0000000000000001e-05</span>)) <span style=\"color: #AA22FF; font-weight: bold\">*</span> BN1_weight[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> BN1_bias[v_c]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w, rph, rpw <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Pooled&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w, v_rph, v_rpw <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRR&quot;</span>, [n, c, h, w, rph, rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(BatchNorm[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Pooled[v_n, v_c, v_h, v_w])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Pooled[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #AA22FF; font-weight: bold\">-</span><span style=\"color: #008000\">340282346638528859811704183484516925440.0</span>)\n",
              "                Pooled[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Pooled[v_n, v_c, v_h, v_w], BatchNorm[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">18</span>, <span style=\"color: #008000\">18</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;PaddedInput_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Pooled[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(PaddedInput_1[v_n, v_c, v_h, v_w])\n",
              "                PaddedInput_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_h <span style=\"color: #008000; font-weight: bold\">and</span> v_h <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">17</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_w <span style=\"color: #008000; font-weight: bold\">and</span> v_w <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">17</span>, Pooled[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q, rc, ry, rx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Conv_1&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q, v_rc, v_ry, v_rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [n, k, p, q, rc, ry, rx])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(PaddedInput_1[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx], Kernel2[v_k, v_rc, v_ry, v_rx])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Conv_1[v_n, v_k, v_p, v_q])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> PaddedInput_1[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Kernel2[v_k, v_rc, v_ry, v_rx]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ConvBias_1&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, k, p, q])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Conv_1[v_n, v_k, v_p, v_q], Bias_conv_2[v_k])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ConvBias_1[v_n, v_k, v_p, v_q])\n",
              "                ConvBias_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias_conv_2[v_k]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ReLU_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ConvBias_1[v_n, v_c, v_h, v_w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ReLU_1[v_n, v_c, v_h, v_w])\n",
              "                ReLU_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(ConvBias_1[v_n, v_c, v_h, v_w], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;BatchNorm_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ReLU_1[v_n, v_c, v_h, v_w], BN2_mean[v_c], BN2_var[v_c], BN2_weight[v_c], BN2_bias[v_c])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(BatchNorm_1[v_n, v_c, v_h, v_w])\n",
              "                BatchNorm_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> (ReLU_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">-</span> BN2_mean[v_c]) <span style=\"color: #AA22FF; font-weight: bold\">/</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>sqrt(BN2_var[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">1.0000000000000001e-05</span>)) <span style=\"color: #AA22FF; font-weight: bold\">*</span> BN2_weight[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> BN2_bias[v_c]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w, rph, rpw <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Pooled_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w, v_rph, v_rpw <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRR&quot;</span>, [n, c, h, w, rph, rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(BatchNorm_1[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Pooled_1[v_n, v_c, v_h, v_w])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Pooled_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #AA22FF; font-weight: bold\">-</span><span style=\"color: #008000\">340282346638528859811704183484516925440.0</span>)\n",
              "                Pooled_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Pooled_1[v_n, v_c, v_h, v_w], BatchNorm_1[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, i <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2048</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Flattened&quot;</span>):\n",
              "                v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Pooled_1[v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">64</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span>])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Flattened[v_n, v_i])\n",
              "                Flattened[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Pooled_1[v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">64</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span>]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o, r <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">2048</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Linear&quot;</span>):\n",
              "                v_n, v_o, v_r <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [n, o, r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Flattened[v_n, v_r], Weight1[v_o, v_r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Linear[v_n, v_o])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Flattened[v_n, v_r] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Weight1[v_o, v_r]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;LinearReLU&quot;</span>):\n",
              "                v_n, v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Linear[v_n, v_o], Bias1[v_o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(LinearReLU[v_n, v_o])\n",
              "                LinearReLU[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias1[v_o], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o, r <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Linear_1&quot;</span>):\n",
              "                v_n, v_o, v_r <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [n, o, r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(LinearReLU[v_n, v_r], Weight2[v_o, v_r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Linear_1[v_n, v_o])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> LinearReLU[v_n, v_r] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Weight2[v_o, v_r]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;LinearBias&quot;</span>):\n",
              "                v_n, v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Linear_1[v_n, v_o], Bias2[v_o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(LinearBias[v_n, v_o])\n",
              "                LinearBias[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias2[v_o]\n",
              "</pre></div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "te_func = te.create_prim_func([Input, Kernel1, Kernel2, Bias_conv1, Bias_conv2,\n",
        "                        Weight1, Bias1, Weight2, Bias2,\n",
        "                        BN1_weight, BN1_bias, BN1_mean, BN1_var,\n",
        "                        BN2_weight, BN2_bias, BN2_mean, BN2_var, fc2]).with_attr({\"global_symbol\": \"gemm_original\"})\n",
        "Original_Module_GEMM = tvm.IRModule({\"gemm_original\": te_func})\n",
        "Original_Module_GEMM.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF7pqzv5iAis"
      },
      "source": [
        "Compile & build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "scESdgFoheEb"
      },
      "outputs": [],
      "source": [
        "module = tvm.build(Original_Module_GEMM, [Input, Kernel1, Kernel2, Bias_conv1, Bias_conv2,\n",
        "                        Weight1, Bias1, Weight2, Bias2,\n",
        "                        BN1_weight, BN1_bias, BN1_mean, BN1_var,\n",
        "                        BN2_weight, BN2_bias, BN2_mean, BN2_var, fc2], target=\"llvm\", name=\"gemm_original\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqh2dj0piLEJ"
      },
      "source": [
        "## Inference & Testing Correctness for Original Module GEMM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoNC688RimDv"
      },
      "source": [
        "### Inference & Testing Correctness: `Accuracy %` on Test Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFqBkQiLzaDJ"
      },
      "source": [
        "Define the inference function to calculate the accuracy for the test dataset (CIFAR-10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_DiTvq9wihFv",
        "outputId": "3aa7a7b9-a2bd-4fd1-8d2c-fcffa3834a80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Accuracy is : 79.17 %\n"
          ]
        }
      ],
      "source": [
        "def inference(module, test_loader, config, output_tvm):\n",
        "  ctx = tvm.cpu(0)\n",
        "  num_samples = 0\n",
        "  num_correct = 0\n",
        "\n",
        "  for inputs, targets in test_loader:\n",
        "    input_tvm = tvm.nd.array(inputs, ctx)\n",
        "    module(input_tvm, *config, output_tvm)\n",
        "    output_numpy = output_tvm.asnumpy()\n",
        "    num_samples += targets.size(0)\n",
        "    num_correct += (np.argmax(output_numpy[0]) == targets.numpy()[0]).sum()\n",
        "\n",
        "  print(f\"The Accuracy is : {(num_correct / num_samples * 100).item()} %\")\n",
        "  return input_tvm\n",
        "\n",
        "input_tvm = inference(module, test_loader, config, output_tvm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHgLwa2Bi4HD"
      },
      "source": [
        "### Inference & Testing Correctness: `Prediction` for Single Instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ql_MAFRUi42v",
        "outputId": "6854f74c-92ca-4b6c-fe14-e36d3c0af1a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Module GEMM's Prediction is: cat\n"
          ]
        }
      ],
      "source": [
        "input_tvm = tvm.nd.array(img, ctx)\n",
        "input_tvm.shape\n",
        "module(input_tvm, *config, output_tvm)\n",
        "pred_kind = np.argmax(output_tvm.numpy(), axis=1)\n",
        "print(\"Original Module GEMM's Prediction is:\", class_names[pred_kind[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjkt-MGhjXt0"
      },
      "source": [
        "### Inference & Testing Correctness: `Assertion` Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8v6RP7oIjYLJ",
        "outputId": "c763b5b5-0550-4a7a-e582-ec48aa0bafb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assertion Test Passed.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    np.testing.assert_allclose(pred.numpy(), output_tvm.asnumpy(), rtol=1e-4)\n",
        "    print(\"Assertion Test Passed.\")\n",
        "except AssertionError as e:\n",
        "    print(f\"AssertionError: {e}\")\n",
        "    print(\"Assertion Test Failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8sp4vxDjqfk"
      },
      "source": [
        "## Execution Time for Original GEMM Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM34laYKjesT",
        "outputId": "c944cb50-5951-49c3-edfc-58e47ab8000a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time for Original Module GEMM is: 2.153 ms\n"
          ]
        }
      ],
      "source": [
        "def evaluate_timer(module, input_tvm, config, output_tvm):\n",
        "  eval = module.time_evaluator(module.entry_name, tvm.cpu(), number=10)\n",
        "  mean_time = eval(input_tvm, *config, output_tvm).mean\n",
        "  return mean_time\n",
        "\n",
        "original_module_et = evaluate_timer(module, input_tvm, config, output_tvm)\n",
        "print(\"Execution Time for Original Module GEMM is: %.3f ms\" % (original_module_et * 1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wpPvIIekHRO"
      },
      "source": [
        "## Optimizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smJfWPSVkcxl"
      },
      "source": [
        "### Perform optimizations like `Reordering`, `Vectorization` and `Parallelization` on `Original_Module_GEMM`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "03Nke4clkJbg",
        "outputId": "a53b8db9-3cb4-44d8-c0ee-dc1599b7c0bb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">gemm_original</span>(Input: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Kernel1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Kernel2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias_conv_1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias_conv_2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Weight1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">2048</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias1: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Weight2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), Bias2: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">10</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_weight: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_bias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_mean: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN1_var: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_weight: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_bias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_mean: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), BN2_var: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">32</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), LinearBias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        PaddedInput <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">34</span>, <span style=\"color: #008000\">34</span>))\n",
              "        Conv <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        ConvBias <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        ReLU <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        BatchNorm <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>))\n",
              "        Pooled <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        PaddedInput_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">18</span>, <span style=\"color: #008000\">18</span>))\n",
              "        Conv_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        ConvBias_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        ReLU_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        BatchNorm_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>))\n",
              "        Pooled_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">8</span>))\n",
              "        Flattened <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2048</span>))\n",
              "        Linear <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>))\n",
              "        LinearReLU <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>))\n",
              "        Linear_1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">34</span>, <span style=\"color: #008000\">34</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;PaddedInput&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Input[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(PaddedInput[v_n, v_c, v_h, v_w])\n",
              "                PaddedInput[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_h <span style=\"color: #008000; font-weight: bold\">and</span> v_h <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">33</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_w <span style=\"color: #008000; font-weight: bold\">and</span> v_w <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">33</span>, Input[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q, rc, ry, rx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Conv&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q, v_rc, v_ry, v_rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [n, k, p, q, rc, ry, rx])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(PaddedInput[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx], Kernel1[v_k, v_rc, v_ry, v_rx])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Conv[v_n, v_k, v_p, v_q])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> PaddedInput[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Kernel1[v_k, v_rc, v_ry, v_rx]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ConvBias&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, k, p, q])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Conv[v_n, v_k, v_p, v_q], Bias_conv_1[v_k])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ConvBias[v_n, v_k, v_p, v_q])\n",
              "                ConvBias[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias_conv_1[v_k]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ReLU&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ConvBias[v_n, v_c, v_h, v_w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ReLU[v_n, v_c, v_h, v_w])\n",
              "                ReLU[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(ConvBias[v_n, v_c, v_h, v_w], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">32</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;BatchNorm&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ReLU[v_n, v_c, v_h, v_w], BN1_mean[v_c], BN1_var[v_c], BN1_weight[v_c], BN1_bias[v_c])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(BatchNorm[v_n, v_c, v_h, v_w])\n",
              "                BatchNorm[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> (ReLU[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">-</span> BN1_mean[v_c]) <span style=\"color: #AA22FF; font-weight: bold\">/</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>sqrt(BN1_var[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">1.0000000000000001e-05</span>)) <span style=\"color: #AA22FF; font-weight: bold\">*</span> BN1_weight[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> BN1_bias[v_c]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w, rph, rpw <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Pooled&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w, v_rph, v_rpw <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRR&quot;</span>, [n, c, h, w, rph, rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(BatchNorm[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Pooled[v_n, v_c, v_h, v_w])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Pooled[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #AA22FF; font-weight: bold\">-</span><span style=\"color: #008000\">340282346638528859811704183484516925440.0</span>)\n",
              "                Pooled[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Pooled[v_n, v_c, v_h, v_w], BatchNorm[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">18</span>, <span style=\"color: #008000\">18</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;PaddedInput_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Pooled[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(PaddedInput_1[v_n, v_c, v_h, v_w])\n",
              "                PaddedInput_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>if_then_else(<span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_h <span style=\"color: #008000; font-weight: bold\">and</span> v_h <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">17</span> <span style=\"color: #008000; font-weight: bold\">and</span> <span style=\"color: #008000\">1</span> <span style=\"color: #AA22FF; font-weight: bold\">&lt;=</span> v_w <span style=\"color: #008000; font-weight: bold\">and</span> v_w <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">17</span>, Pooled[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>, v_w <span style=\"color: #AA22FF; font-weight: bold\">-</span> <span style=\"color: #008000\">1</span>], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q, rc, ry, rx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">3</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Conv_1&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q, v_rc, v_ry, v_rx <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRRR&quot;</span>, [n, k, p, q, rc, ry, rx])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(PaddedInput_1[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx], Kernel2[v_k, v_rc, v_ry, v_rx])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Conv_1[v_n, v_k, v_p, v_q])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> PaddedInput_1[v_n, v_rc, v_p <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_ry, v_q <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rx] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Kernel2[v_k, v_rc, v_ry, v_rx]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, k, p, q <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ConvBias_1&quot;</span>):\n",
              "                v_n, v_k, v_p, v_q <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, k, p, q])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Conv_1[v_n, v_k, v_p, v_q], Bias_conv_2[v_k])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ConvBias_1[v_n, v_k, v_p, v_q])\n",
              "                ConvBias_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Conv_1[v_n, v_k, v_p, v_q] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias_conv_2[v_k]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> h_0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">8</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> w_0, h_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">2</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">for</span> w_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">2</span>):\n",
              "                        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;ReLU_1&quot;</span>):\n",
              "                            v_n, v_c <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, c])\n",
              "                            v_h <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, h_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> h_1)\n",
              "                            v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">16</span>, w_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> w_1)\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ConvBias_1[v_n, v_c, v_h, v_w])\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(ReLU_1[v_n, v_c, v_h, v_w])\n",
              "                            ReLU_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(ConvBias_1[v_n, v_c, v_h, v_w], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">16</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;BatchNorm_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSS&quot;</span>, [n, c, h, w])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(ReLU_1[v_n, v_c, v_h, v_w], BN2_mean[v_c], BN2_var[v_c], BN2_weight[v_c], BN2_bias[v_c])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(BatchNorm_1[v_n, v_c, v_h, v_w])\n",
              "                BatchNorm_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> (ReLU_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">-</span> BN2_mean[v_c]) <span style=\"color: #AA22FF; font-weight: bold\">/</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>sqrt(BN2_var[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">1.0000000000000001e-05</span>)) <span style=\"color: #AA22FF; font-weight: bold\">*</span> BN2_weight[v_c] <span style=\"color: #AA22FF; font-weight: bold\">+</span> BN2_bias[v_c]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, c, h, w, rph, rpw <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">32</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">2</span>, <span style=\"color: #008000\">2</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Pooled_1&quot;</span>):\n",
              "                v_n, v_c, v_h, v_w, v_rph, v_rpw <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSSSRR&quot;</span>, [n, c, h, w, rph, rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(BatchNorm_1[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Pooled_1[v_n, v_c, v_h, v_w])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Pooled_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #AA22FF; font-weight: bold\">-</span><span style=\"color: #008000\">340282346638528859811704183484516925440.0</span>)\n",
              "                Pooled_1[v_n, v_c, v_h, v_w] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Pooled_1[v_n, v_c, v_h, v_w], BatchNorm_1[v_n, v_c, v_h <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rph, v_w <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_rpw])\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, i <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">2048</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Flattened&quot;</span>):\n",
              "                v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Pooled_1[v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">64</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span>])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Flattened[v_n, v_i])\n",
              "                Flattened[v_n, v_i] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Pooled_1[v_n, v_i <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">64</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span>, v_i <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span>]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o, r <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">2048</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Linear&quot;</span>):\n",
              "                v_n, v_o, v_r <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [n, o, r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Flattened[v_n, v_r], Weight1[v_o, v_r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Linear[v_n, v_o])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Flattened[v_n, v_r] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Weight1[v_o, v_r]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;LinearReLU&quot;</span>):\n",
              "                v_n, v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Linear[v_n, v_o], Bias1[v_o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(LinearReLU[v_n, v_o])\n",
              "                LinearReLU[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Linear[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias1[v_o], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o, r <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">128</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Linear_1&quot;</span>):\n",
              "                v_n, v_o, v_r <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [n, o, r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(LinearReLU[v_n, v_r], Weight2[v_o, v_r])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Linear_1[v_n, v_o])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
              "                Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> LinearReLU[v_n, v_r] <span style=\"color: #AA22FF; font-weight: bold\">*</span> Weight2[v_o, v_r]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n, o <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;LinearBias&quot;</span>):\n",
              "                v_n, v_o <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Linear_1[v_n, v_o], Bias2[v_o])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(LinearBias[v_n, v_o])\n",
              "                LinearBias[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Linear_1[v_n, v_o] <span style=\"color: #AA22FF; font-weight: bold\">+</span> Bias2[v_o]\n",
              "</pre></div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sch_manual_optim = tvm.tir.Schedule(Original_Module_GEMM)\n",
        "sch_manual_optim.work_on(\"gemm_original\")\n",
        "\n",
        "block_relu = sch_manual_optim.get_block(\"ReLU_1\")\n",
        "n, c, h, w = sch_manual_optim.get_loops(block_relu)\n",
        "\n",
        "h_outer, h_inner = sch_manual_optim.split(h, factors=[8, 2])\n",
        "w_outer, w_inner = sch_manual_optim.split(w, factors=[8, 2])\n",
        "\n",
        "# Reordering\n",
        "sch_manual_optim.reorder(n, c, h_outer, w_outer, h_inner, w_inner)\n",
        "\n",
        "# Vectorization\n",
        "sch_manual_optim.vectorize(w_inner)\n",
        "\n",
        "# Parallelization\n",
        "sch_manual_optim.parallel(h_outer)\n",
        "\n",
        "sch_manual_optim.mod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hHd8r5AlCJx"
      },
      "source": [
        "### Manual Optimization GEMM Build & Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk9ZtE8_lC58"
      },
      "source": [
        "#### Build `manually_optimized_module_gemm`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h8iJx1xilPvs"
      },
      "outputs": [],
      "source": [
        "manual_optimized_module_gemm = tvm.build(sch_manual_optim.mod, [Input, Kernel1, Kernel2, Bias_conv1, Bias_conv2,\n",
        "                        Weight1, Bias1, Weight2, Bias2,\n",
        "                        BN1_weight, BN1_bias, BN1_mean, BN1_var,\n",
        "                        BN2_weight, BN2_bias, BN2_mean, BN2_var, fc2], target=\"llvm\", name=\"cnn_manual_optimized_gemm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JVKIzE-yTqa"
      },
      "source": [
        "#### Inference & Testing Correctness of Manual Optimization Module GEMM\n",
        " - Accuracy %\n",
        " - Prediction\n",
        " - Assertion Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "16RXBIpCzotN",
        "outputId": "efca8ad5-4723-4a35-8dc6-67b08f858ac2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Accuracy is : 79.17 %\n",
            "Manual Optimization Module GEMM's Prediction is: cat\n",
            "Assertion Test Passed for Manual Optimization GEMM.\n"
          ]
        }
      ],
      "source": [
        "output_tvm_manual_gemm = tvm.nd.empty((N, 10), device=ctx)\n",
        "input_tvm_manual_gemm = inference(manual_optimized_module_gemm, test_loader, config, output_tvm_manual_gemm)\n",
        "input_tvm_manual_gemm = tvm.nd.array(img, ctx)\n",
        "input_tvm_manual_gemm.shape\n",
        "manual_optimized_module_gemm(input_tvm_manual_gemm, *config, output_tvm_manual_gemm)\n",
        "pred_kind = np.argmax(output_tvm_manual_gemm.numpy(), axis=1)\n",
        "print(\"Manual Optimization Module GEMM's Prediction is:\", class_names[pred_kind[0]])\n",
        "try:\n",
        "    np.testing.assert_allclose(pred.numpy(), output_tvm_manual_gemm.asnumpy(), rtol=1e-4)\n",
        "    print(\"Assertion Test Passed for Manual Optimization GEMM.\")\n",
        "except AssertionError as e:\n",
        "    print(f\"AssertionError: {e}\")\n",
        "    print(\"Assertion Test Failed for Manual Optimization GEMM.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK_ackPTmkMj"
      },
      "source": [
        "#### Evaluate execution time of Manual Optimization Module GEMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdCrCwILmfct",
        "outputId": "da9ed2d7-2dda-4102-aa54-21ea44ba2d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time for Manual Optimization Module GEMM is : 2.388 ms\n"
          ]
        }
      ],
      "source": [
        "manual_optimized_module_gemm_et = evaluate_timer(manual_optimized_module_gemm, input_tvm, config, output_tvm)\n",
        "print(\"Execution Time for Manual Optimization Module GEMM is : %.3f ms\" % (manual_optimized_module_gemm_et * 1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcrxg4b7lt4Z"
      },
      "source": [
        "### Auto Optimization GEMM Build & Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wxgCCKnmlvCd",
        "outputId": "426480f0-e773-404c-cd38-cdac4404b513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-10-04 00:34:25 [INFO] Logging directory: ./tune_tmp/logs\n",
            "2024-10-04 00:34:25 [INFO] LocalBuilder: max_workers = 1\n",
            "2024-10-04 00:34:27 [INFO] LocalRunner: max_workers = 1\n",
            "2024-10-04 00:34:33 [INFO] [task_scheduler.cc:159] Initializing Task #0: \"gemm_original\"\n",
            "2024-10-04 00:35:18 [INFO] [task_scheduler.cc:320] \n",
            " ID |          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
            "---------------------------------------------------------------------------------------------------------------\n",
            "  0 | gemm_original | 3967754 |      1 |            N/A |          N/A |                   N/A |      0 |      \n",
            "---------------------------------------------------------------------------------------------------------------\n",
            "Total trials: 0\n",
            "Total latency (us): 0\n",
            "\n",
            "2024-10-04 00:35:18 [INFO] [task_scheduler.cc:180] TaskScheduler picks Task #0: \"gemm_original\"\n",
            "2024-10-04 00:39:40 [INFO] [task_scheduler.cc:193] Sending 64 sample(s) to builder\n",
            "2024-10-04 00:45:08 [INFO] [task_scheduler.cc:195] Sending 64 sample(s) to runner\n",
            "2024-10-04 00:45:30 [DEBUG] XGB iter   0: tr-p-rmse: 9.235546\ttr-a-peak@32: 0.850846\ttr-rmse: 1.226404\ttr-rmse: 1.226404\n",
            "2024-10-04 00:45:30 [DEBUG] XGB iter  25: tr-p-rmse: 799660.328111\ttr-a-peak@32: 0.728107\ttr-rmse: 62139.557736\ttr-rmse: 62139.557736\n",
            "2024-10-04 00:45:30 [DEBUG] XGB iter  50: tr-p-rmse: 68705133481.251045\ttr-a-peak@32: 0.850846\ttr-rmse: 5338942472.166632\ttr-rmse: 5338942472.166632\n",
            "2024-10-04 00:45:30 [DEBUG] XGB stopped. Best iteration: [0] tr-p-rmse:9.23555\ttr-a-peak@32:0.85085\ttr-rmse:1.22640\ttr-rmse:1.22640 \n",
            "2024-10-04 00:45:31 [INFO] [task_scheduler.cc:237] [Updated] Task #0: \"gemm_original\"\n",
            "2024-10-04 00:45:31 [INFO] [task_scheduler.cc:320] \n",
            " ID |          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
            "---------------------------------------------------------------------------------------------------------------\n",
            "  0 | gemm_original | 3967754 |      1 |         9.0001 |     440.8583 |              440.8583 |     64 |      \n",
            "---------------------------------------------------------------------------------------------------------------\n",
            "Total trials: 64\n",
            "Total latency (us): 440.858\n",
            "\n",
            "2024-10-04 00:45:31 [INFO] [task_scheduler.cc:260] Task #0 has finished. Remaining task(s): 0\n",
            "2024-10-04 00:45:31 [INFO] [task_scheduler.cc:320] \n",
            " ID |          Name |    FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Done \n",
            "---------------------------------------------------------------------------------------------------------------\n",
            "  0 | gemm_original | 3967754 |      1 |         9.0001 |     440.8583 |              440.8583 |     64 |    Y \n",
            "---------------------------------------------------------------------------------------------------------------\n",
            "Total trials: 64\n",
            "Total latency (us): 440.858\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tvm import meta_schedule as ms\n",
        "\n",
        "database = ms.tune_tir(\n",
        "    mod=Original_Module_GEMM,\n",
        "    target=\"llvm --num-cores=1\",\n",
        "    max_trials_global=64,\n",
        "    num_trials_per_iter=64,\n",
        "    work_dir=\"./tune_tmp\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0ozC8JjmEPq"
      },
      "source": [
        "#### Build `auto_optimized_module_gemm`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FF2UdqzPmE83"
      },
      "outputs": [],
      "source": [
        "auto_optimized_module_gemm = tvm.build(sch_tuned.mod, [Input, Kernel1, Kernel2, Bias_conv1, Bias_conv2,\n",
        "                        Weight1, Bias1, Weight2, Bias2,\n",
        "                        BN1_weight, BN1_bias, BN1_mean, BN1_var,\n",
        "                        BN2_weight, BN2_bias, BN2_mean, BN2_var, fc2], target=\"llvm\", name=\"cnn_auto_optimized_gemm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bP8GquCyNYW"
      },
      "source": [
        "#### Testing Correctness of Auto Optimization Module GEMM\n",
        " - Accuracy %\n",
        " - Prediction\n",
        " - Assertion Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ePMisIVjznb1",
        "outputId": "80cb063e-ac50-4cbb-f2a3-2dcfa9f4df40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Accuracy is : 78.44 %\n",
            "Auto Optimization Module GEMM's Prediction is: cat\n",
            "Assertion Test Passed for Auto Optimization GEMM.\n"
          ]
        }
      ],
      "source": [
        "output_tvm_auto_gemm = tvm.nd.empty((N, 10), device=ctx)\n",
        "input_tvm_auto_gemm = inference(auto_optimized_module_gemm, test_loader, config, output_tvm_auto_gemm)\n",
        "input_tvm_auto_gemm = tvm.nd.array(img, ctx)\n",
        "input_tvm_auto_gemm.shape\n",
        "auto_optimized_module_gemm(input_tvm_auto_gemm, *config, output_tvm_auto_gemm)\n",
        "pred_kind = np.argmax(output_tvm_auto_gemm.numpy(), axis=1)\n",
        "print(\"Auto Optimization Module GEMM's Prediction is:\", class_names[pred_kind[0]])\n",
        "try:\n",
        "    np.testing.assert_allclose(pred.numpy(), output_tvm_auto_gemm.asnumpy(), rtol=1e-1, atol=0.5)\n",
        "    print(\"Assertion Test Passed for Auto Optimization GEMM.\")\n",
        "except AssertionError as e:\n",
        "    print(f\"AssertionError: {e}\")\n",
        "    print(\"Assertion Test Failed for Auto Optimization GEMM.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41UDnTZNmVx9"
      },
      "source": [
        "#### Evaluate Execution time of Auto Optimization Module GEMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QxFW5HuqmY6I",
        "outputId": "3fcf2765-3f27-497b-9ebc-6bfa52b8be2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time for Auto Optimization Module GEMM is: 0.486 ms\n"
          ]
        }
      ],
      "source": [
        "auto_optimized_module_gemm_et = evaluate_timer(auto_optimized_module_gemm, input_tvm, config, output_tvm)\n",
        "print(\"Execution Time for Auto Optimization Module GEMM is: %.3f ms\" % (auto_optimized_module_gemm_et * 1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SggEPEFN3erL"
      },
      "source": [
        "#3. Summary & Result Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Tabular Comparison of Execution Time` ->\n",
        "\n",
        "\n",
        "|    Method               |    Original    |    Manual Optimization    |    Auto Optimization    |\n",
        "|:---------------------:|:------------:|:-------------------------:|:-----------------------:|\n",
        "| Convolution Method        |    2.406     |           2.245        |          0.680          |\n",
        "| GEMM Method   |    2.153     |           2.388           |          0.486          |\n",
        "\n",
        "(unit of time here is ms)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### `Tabular Comparison of Accuracy` ->\n",
        "\n",
        "\n",
        "|    Method               |    Original    |    Manual Optimization    |    Auto Optimization    |\n",
        "|:---------------------:|:------------:|:-------------------------:|:-----------------------:|\n",
        "| Convolution Method        |    79.17     |           79.17        |          78.44          |\n",
        "| GEMM Method   |    79.17     |           79.17          |       78.44         |\n",
        "\n",
        "(unit of accuracy here is %)\n",
        "\n",
        "\n",
        "### `Result Analysis` ->\n",
        "\n",
        "- **Execution Time Analysis** :\n",
        "\n",
        "  - The original implementation of the convolution method took 2.406 ms to execute. Manual optimizations slightly improved the execution time to 2.245 ms, indicating some performance enhancement. However, the most significant improvement is seen in the auto optimization, reducing the time to 0.680 ms. This represents a 72% reduction in execution time compared to the original method and a 69.7% improvement over the manually optimized version.\n",
        "\n",
        "  - The GEMM method's original execution time was 2.153 ms. Interestingly, manual optimization actually increased the time to 2.388 ms, suggesting that the manual tuning was suboptimal for this method. In contrast, auto optimization achieved a drastic improvement, reducing the execution time to 0.486 ms. This is a 77.4% reduction from the original and an 79.6% improvement over manual optimization.\n",
        "\n",
        "- **Accuracy Analysis**\n",
        "\n",
        "  - The accuracy for the convolution method remained constant at 79.17% for both the original and manually optimized implementations. However, there was a slight decrease in accuracy to 78.44% with auto optimization, showing a small drop of 0.73%.\n",
        "  - Similar to the convolution method, the GEMM method maintained 79.17% accuracy for both the original and manually optimized versions. Auto optimization also resulted in a slight drop to 78.44%, identical to the convolution method, with a 0.73% decrease in accuracy.\n",
        "\n",
        "### `Summary` ->\n",
        "- **Execution Time Improvements**: The most notable observation is the impressive performance gains achieved through auto optimization for both methods. For the convolution method, the execution time was reduced by 72%, and for the GEMM method, the reduction was 77.4%. These improvements highlight the effectiveness of automatic optimization techniques over manual tuning.\n",
        "\n",
        "- **Accuracy Trade-off**: While auto optimization significantly enhanced execution speed, it came at a slight cost in accuracy for both methods, with a 0.73% drop. However, this decrease may be acceptable depending on the use case, particularly in performance-critical scenarios where speed is prioritized.\n",
        "\n",
        "- Auto optimization has proven to be the most effective approach for improving execution times in both the convolution and GEMM methods, though there is a small trade-off in accuracy."
      ],
      "metadata": {
        "id": "dxizFomjHvED"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NMAxhe5YIj8w",
        "jTikZYlqefFQ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}